{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20368ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import warnings\n",
    "from statistics import mean\n",
    "\n",
    "PAD_TOKEN=0\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be059ede",
   "metadata": {},
   "source": [
    "# 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de211b5",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b187b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb03e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_tmp_train_raw = load_data(os.path.join('data','ATIS','train.json'))\n",
    "ATIS_test_raw = load_data(os.path.join('data','ATIS','test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae958228",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPS_train_raw = load_data(os.path.join('data','SNIPS','train.json'))\n",
    "SNIPS_dev_raw= load_data(os.path.join('data','SNIPS','valid.json'))\n",
    "SNIPS_test_raw = load_data(os.path.join('data','SNIPS','test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0c68b",
   "metadata": {},
   "source": [
    "### Sentence length analysis\n",
    "Visualizing the max length and the length distribution of the dataset, will be useful for setting max_length parameter in the Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77fb9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(samples_count,sent_length):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.xticks(sent_length)\n",
    "    plt.bar(sent_length,samples_count,width=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69abc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length=50\n",
    "ATIS_count=[]\n",
    "SNIPS_count=[]\n",
    "total_count=[]\n",
    "sent_length=[]\n",
    "for i in range(0,plot_length+1):\n",
    "    ATIS_count.append(0)\n",
    "    SNIPS_count.append(0)\n",
    "    total_count.append(0)\n",
    "    sent_length.append(i)\n",
    "\n",
    "ATIS_dataset=ATIS_tmp_train_raw+ATIS_test_raw\n",
    "SNIPS_dataset=SNIPS_train_raw+SNIPS_dev_raw+SNIPS_test_raw\n",
    "total_dataset=ATIS_dataset+SNIPS_dataset\n",
    "\n",
    "for sample in ATIS_dataset:\n",
    "    utterance=sample['utterance'].split()\n",
    "    sent_len=len(utterance)\n",
    "    ATIS_count[sent_len]+=1\n",
    "\n",
    "plot_count(ATIS_count,sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a94dda6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in SNIPS_dataset:\n",
    "    utterance=sample['utterance'].split()\n",
    "    sent_len=len(utterance)\n",
    "    SNIPS_count[sent_len]+=1\n",
    "\n",
    "plot_count(SNIPS_count,sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6def38b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in total_dataset:\n",
    "    utterance=sample['utterance'].split()\n",
    "    sent_len=len(utterance)\n",
    "    total_count[sent_len]+=1\n",
    "\n",
    "plot_count(total_count,sent_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baee05",
   "metadata": {},
   "source": [
    "### Create development set\n",
    "\n",
    "As suggested in class, we create a development set used for hyperparameter tuning using a stratification strategy in order to preserve the intents data distributions.\n",
    "Since SNIPS dataset already comes with a validation set, the development set for this dataset is not created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425cb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
    "# on the training set which is around 11% \n",
    "portion = round(((len(ATIS_tmp_train_raw) + len(ATIS_test_raw)) * 0.10)/(len(ATIS_tmp_train_raw)),2)\n",
    "\n",
    "intents = [x['intent'] for x in ATIS_tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)#For each class count the appearances\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # Some intents have only one instance, we put them in training\n",
    "        X.append(ATIS_tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(ATIS_tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "ATIS_X_train, ATIS_X_dev, ATIS_y_train, ATIS_y_dev = train_test_split(X, Y, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "ATIS_X_train.extend(mini_Train)\n",
    "ATIS_train_raw = ATIS_X_train\n",
    "ATIS_dev_raw = ATIS_X_dev\n",
    "\n",
    "ATIS_y_test = [x['intent'] for x in ATIS_test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(ATIS_y_train),3)*100 for k, v in sorted(Counter(ATIS_y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(ATIS_y_dev),3)*100 for k, v in sorted(Counter(ATIS_y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(ATIS_y_test),3)*100 for k, v in sorted(Counter(ATIS_y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(ATIS_train_raw))\n",
    "print('DEV size:', len(ATIS_dev_raw))\n",
    "print('TEST size:', len(ATIS_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ffe521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIPS\n",
      "TRAIN size: 13084\n",
      "DEV size: 700\n",
      "TEST size: 700\n",
      "Train:\n",
      "{'AddToPlaylist': 13.900000000000002,\n",
      " 'BookRestaurant': 14.399999999999999,\n",
      " 'GetWeather': 14.499999999999998,\n",
      " 'PlayMusic': 14.6,\n",
      " 'RateBook': 14.299999999999999,\n",
      " 'SearchCreativeWork': 14.099999999999998,\n",
      " 'SearchScreeningEvent': 14.2}\n",
      "Dev:\n",
      "{'AddToPlaylist': 14.299999999999999,\n",
      " 'BookRestaurant': 14.299999999999999,\n",
      " 'GetWeather': 14.299999999999999,\n",
      " 'PlayMusic': 14.299999999999999,\n",
      " 'RateBook': 14.299999999999999,\n",
      " 'SearchCreativeWork': 14.299999999999999,\n",
      " 'SearchScreeningEvent': 14.299999999999999}\n",
      "Test:\n",
      "{'AddToPlaylist': 17.7,\n",
      " 'BookRestaurant': 13.100000000000001,\n",
      " 'GetWeather': 14.899999999999999,\n",
      " 'PlayMusic': 12.3,\n",
      " 'RateBook': 11.4,\n",
      " 'SearchCreativeWork': 15.299999999999999,\n",
      " 'SearchScreeningEvent': 15.299999999999999}\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('SNIPS')\n",
    "print('TRAIN size:', len(SNIPS_train_raw))\n",
    "print('DEV size:', len(SNIPS_dev_raw))\n",
    "print('TEST size:', len(SNIPS_test_raw))\n",
    "\n",
    "SNIPS_intent_train=[]\n",
    "SNIPS_intent_dev=[]\n",
    "SNIPS_intent_test=[]\n",
    "\n",
    "for sample in SNIPS_train_raw:\n",
    "    SNIPS_intent_train.append(sample['intent'])\n",
    "\n",
    "for sample in SNIPS_dev_raw:\n",
    "    SNIPS_intent_dev.append(sample['intent'])\n",
    "\n",
    "for sample in SNIPS_test_raw:\n",
    "    SNIPS_intent_test.append(sample['intent'])\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(SNIPS_intent_train),3)*100 for k, v in sorted(Counter(SNIPS_intent_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(SNIPS_intent_dev),3)*100 for k, v in sorted(Counter(SNIPS_intent_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(SNIPS_intent_test),3)*100 for k, v in sorted(Counter(SNIPS_intent_test).items())})\n",
    "print('='*89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8683d",
   "metadata": {},
   "source": [
    "### Lang Class\n",
    "\n",
    "Class handling conversion of words/slots/intents to ids and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33348ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8868bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words: 861\n",
      "# Slots: 129\n",
      "# Intents: 26\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "ATIS_words = sum([x['utterance'].split() for x in ATIS_train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "ATIS_corpus = ATIS_train_raw + ATIS_dev_raw + ATIS_test_raw # We do not want unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "ATIS_slots = set(sum([line['slots'].split() for line in ATIS_corpus],[])) # the type of slots\n",
    "ATIS_intents = set([line['intent'] for line in ATIS_corpus]) #all the intent types\n",
    "\n",
    "ATIS_lang = Lang(ATIS_words, ATIS_intents, ATIS_slots, cutoff=0)\n",
    "\n",
    "print('# Words:', len(ATIS_lang.word2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(ATIS_lang.slot2id)-1)\n",
    "print('# Intents:', len(ATIS_lang.intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4150022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words: 11417\n",
      "# Slots: 72\n",
      "# Intents: 7\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "SNIPS_words = sum([x['utterance'].split() for x in SNIPS_train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "SNIPS_corpus = SNIPS_train_raw + SNIPS_dev_raw + SNIPS_test_raw # We do not want unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "SNIPS_slots = set(sum([line['slots'].split() for line in SNIPS_corpus],[])) # the type of slots\n",
    "SNIPS_intents = set([line['intent'] for line in SNIPS_corpus]) #all the intent types\n",
    "\n",
    "SNIPS_lang = Lang(SNIPS_words, SNIPS_intents, SNIPS_slots, cutoff=0)\n",
    "\n",
    "print('# Words:', len(SNIPS_lang.word2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(SNIPS_lang.slot2id)-1)\n",
    "print('# Intents:', len(SNIPS_lang.intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b7ad5",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e135b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c2ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS_ dataset\n",
    "ATIS_train_dataset = IntentsAndSlots(ATIS_train_raw, ATIS_lang)\n",
    "ATIS_dev_dataset = IntentsAndSlots(ATIS_dev_raw, ATIS_lang)\n",
    "ATIS_test_dataset = IntentsAndSlots(ATIS_test_raw, ATIS_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae96461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNIPS_ dataset\n",
    "SNIPS_train_dataset = IntentsAndSlots(SNIPS_train_raw, SNIPS_lang)\n",
    "SNIPS_dev_dataset = IntentsAndSlots(SNIPS_dev_raw, SNIPS_lang)\n",
    "SNIPS_test_dataset = IntentsAndSlots(SNIPS_test_raw, SNIPS_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f8a01",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8173eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our seleceted device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49afca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS_ Dataloader instantiation\n",
    "ATIS_train_loader = DataLoader(ATIS_train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "ATIS_dev_loader = DataLoader(ATIS_dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "ATIS_test_loader = DataLoader(ATIS_test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf30aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNIPS_ Dataloader instantiation\n",
    "SNIPS_train_loader = DataLoader(SNIPS_train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "SNIPS_dev_loader = DataLoader(SNIPS_dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "SNIPS_test_loader = DataLoader(SNIPS_test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3b3a5",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "Function to randomly initialize the weights of Neural Networks in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd82682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1623406",
   "metadata": {},
   "source": [
    "### Evaluation Loop, Predict and Loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cbb099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    #tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]) if elem != 0 else (utterance[id_el], 'O'))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d9ce7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    return hyp_intents,ref_intents,hyp_slots,ref_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67a7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(sampled_epochs, loss_train, loss_dev):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title('Training Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(sampled_epochs, loss_train, label='Train loss')\n",
    "    plt.plot(sampled_epochs, loss_dev, label='Dev loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e37b78",
   "metadata": {},
   "source": [
    "### Error Analysis Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "058ca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_accuracy(pred,gt):\n",
    "    lenght=len(pred)\n",
    "    correct=0\n",
    "    for i in range(0,lenght):\n",
    "        if(pred[i]==gt[i]):\n",
    "            correct+=1\n",
    "    return correct/lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f29f5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intent accuracy by sentence lenght, provides bar plot with intent accuracy per sentence lenght, useful to detect potential deficits in long sentences\n",
    "def intent_acc_lenght(intent_hyp,intent_gt, ATIS=True):\n",
    "    dataset=\"\"\n",
    "    if(ATIS):\n",
    "        dataset=\"ATIS\"\n",
    "    else:\n",
    "        dataset=\"SNIPS\"\n",
    "    test_results=[]\n",
    "    for i in range(0,50):\n",
    "        test_results.append([])\n",
    "    for i in range(0,len(intent_hyp)):\n",
    "        sent_lenght=len(slot_gt[i])\n",
    "        #print(sent_lenght)\n",
    "        test_results[sent_lenght].append((intent_hyp[i],intent_gt[i]))\n",
    "\n",
    "    #Measure accuracy at each sentence lenght\n",
    "    pred=[]\n",
    "    true=[]\n",
    "    sent_length=[]\n",
    "    accuracy_values=[]\n",
    "    accuracy_lenght=[]\n",
    "    for index in range(0,50):\n",
    "        if(len(test_results[index])!=0):\n",
    "            for (hyp, gt) in test_results[index]:\n",
    "                pred.append(hyp)\n",
    "                true.append(gt)\n",
    "            print(\"Accuracy at sentence lenght \",index,\"=\",intent_accuracy(pred, true))\n",
    "            sent_length.append(index)\n",
    "            accuracy_values.append(intent_accuracy(pred, true))\n",
    "\n",
    "    #Plot results\n",
    "    print(\"=\"*85)\n",
    "    print(dataset,\" Intent Accuracy by sentence length\")\n",
    "    plt.xticks(sent_length)\n",
    "\n",
    "    plt.bar(sent_length,accuracy_values,width=0.8)\n",
    "    plt.show()\n",
    "    #Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea4e83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_confusion_matrix():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c6a9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_intent_acc():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c5c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_intent_acc():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "480e34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_slots():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e607edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_slots():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844d23c",
   "metadata": {},
   "source": [
    "# 2) Baseline Model\n",
    "\n",
    "As a baseline model i took the Neural Network that was presented during lab experience #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "560cad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False)    \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee891591",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bf7c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690959ae",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a00ab76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0174c",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Multiple runs are made to compute accuracy and f1 mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881fa718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [05:28<00:00, 65.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1 0.919 +- 0.005\n",
      "Intent Acc 0.937 +- 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    ATIS_model = ModelIAS(hid_size, ATIS_out_slot, ATIS_out_int, emb_size, ATIS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    ATIS_model.apply(init_weights)\n",
    "    \n",
    "    ATIS_optimizer = optim.Adam(ATIS_model.parameters(), lr=lr)\n",
    "    ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    ATIS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop(ATIS_train_loader, ATIS_optimizer, ATIS_criterion_slots, \n",
    "                      ATIS_criterion_intents, ATIS_model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(ATIS_dev_loader, ATIS_criterion_slots, \n",
    "                                                          ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    ATIS_results_test, ATIS_intent_test, _ = eval_loop(ATIS_test_loader, ATIS_criterion_slots, \n",
    "                                         ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "    intent_acc.append(ATIS_intent_test['accuracy'])\n",
    "    slot_f1s.append(ATIS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83653410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [08:16<00:00, 99.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1 0.803 +- 0.01\n",
      "Intent Acc 0.966 +- 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    SNIPS_model = ModelIAS(hid_size, SNIPS_out_slot, SNIPS_out_int, emb_size, SNIPS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    SNIPS_model.apply(init_weights)\n",
    "    \n",
    "    SNIPS_optimizer = optim.Adam(SNIPS_model.parameters(), lr=lr)\n",
    "    SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    SNIPS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop(SNIPS_train_loader, SNIPS_optimizer, SNIPS_criterion_slots, \n",
    "                      SNIPS_criterion_intents, SNIPS_model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(SNIPS_dev_loader, SNIPS_criterion_slots, \n",
    "                                                          SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    SNIPS_results_test, SNIPS_intent_test, _ = eval_loop(SNIPS_test_loader, SNIPS_criterion_slots, \n",
    "                                         SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "    intent_acc.append(SNIPS_intent_test['accuracy'])\n",
    "    slot_f1s.append(SNIPS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147e644",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be66f7f",
   "metadata": {},
   "source": [
    "### Intent Detection Accuracy by Utterance length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c4ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  2 = 0.5\n",
      "Accuracy at sentence lenght  3 = 0.8181818181818182\n",
      "Accuracy at sentence lenght  4 = 0.9534883720930233\n",
      "Accuracy at sentence lenght  5 = 0.9743589743589743\n",
      "Accuracy at sentence lenght  6 = 0.9323308270676691\n",
      "Accuracy at sentence lenght  7 = 0.9227053140096618\n",
      "Accuracy at sentence lenght  8 = 0.935064935064935\n",
      "Accuracy at sentence lenght  9 = 0.9407582938388626\n",
      "Accuracy at sentence lenght  10 = 0.9512670565302144\n",
      "Accuracy at sentence lenght  11 = 0.9533333333333334\n",
      "Accuracy at sentence lenght  12 = 0.9493293591654247\n",
      "Accuracy at sentence lenght  13 = 0.9504814305364512\n",
      "Accuracy at sentence lenght  14 = 0.9481193255512321\n",
      "Accuracy at sentence lenght  15 = 0.944792973651192\n",
      "Accuracy at sentence lenght  16 = 0.9449101796407186\n",
      "Accuracy at sentence lenght  17 = 0.943089430894309\n",
      "Accuracy at sentence lenght  18 = 0.9421965317919075\n",
      "Accuracy at sentence lenght  19 = 0.9425287356321839\n",
      "Accuracy at sentence lenght  20 = 0.9406392694063926\n",
      "Accuracy at sentence lenght  21 = 0.9390519187358917\n",
      "Accuracy at sentence lenght  22 = 0.9391891891891891\n",
      "Accuracy at sentence lenght  23 = 0.9392575928008999\n",
      "Accuracy at sentence lenght  29 = 0.9394618834080718\n",
      "Accuracy at sentence lenght  30 = 0.9395296752519597\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(ATIS_test_loader, ATIS_criterion_slots,\n",
    "                                              ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "119df065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  3 = 1.0\n",
      "Accuracy at sentence lenght  4 = 0.9142857142857143\n",
      "Accuracy at sentence lenght  5 = 0.963855421686747\n",
      "Accuracy at sentence lenght  6 = 0.9503105590062112\n",
      "Accuracy at sentence lenght  7 = 0.9612403100775194\n",
      "Accuracy at sentence lenght  8 = 0.9567723342939481\n",
      "Accuracy at sentence lenght  9 = 0.9624413145539906\n",
      "Accuracy at sentence lenght  10 = 0.9650924024640657\n",
      "Accuracy at sentence lenght  11 = 0.9656419529837251\n",
      "Accuracy at sentence lenght  12 = 0.9679595278246206\n",
      "Accuracy at sentence lenght  13 = 0.9681020733652312\n",
      "Accuracy at sentence lenght  14 = 0.9662058371735791\n",
      "Accuracy at sentence lenght  15 = 0.9669669669669669\n",
      "Accuracy at sentence lenght  16 = 0.967741935483871\n",
      "Accuracy at sentence lenght  17 = 0.9666182873730044\n",
      "Accuracy at sentence lenght  18 = 0.9667630057803468\n",
      "Accuracy at sentence lenght  19 = 0.9668587896253602\n",
      "Accuracy at sentence lenght  20 = 0.9670014347202296\n",
      "Accuracy at sentence lenght  21 = 0.9670487106017192\n",
      "Accuracy at sentence lenght  22 = 0.9670958512160229\n",
      "Accuracy at sentence lenght  24 = 0.9671428571428572\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNIPS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(SNIPS_test_loader, SNIPS_criterion_slots,\n",
    "                                              SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0087c2",
   "metadata": {},
   "source": [
    "# 3) Second Model - Bi-Directional Encoder with GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb44b74",
   "metadata": {},
   "source": [
    "The idea of this second model is to take the architecture of the baseline and apply improvements.\n",
    "The overall structure is still the same, with a joint encoder used for both Slot-Filling and Intent-Detection task and apply two separate classifiers at the end, one for each task.\n",
    "\n",
    "However, the 1-layer, monodirectional LSTM has been replaced by a bi-directional 2 layer GRU.\n",
    "The structure of the GRU implies a reduction in trainable parameters compared to LSTM, speeding up the training process and reducing the risk of overfitting.\n",
    "\n",
    "Thanks to the speeding up in training time, a layer was added to the encoder that resulted in an increase in accuracy.\n",
    "Deeper architecture with more layers were tried but were much slower in training and did not produce improvements during the evaluation compared to the 2 layer model.\n",
    "\n",
    "Also, a change in the loss computation is made:\n",
    "\n",
    "$Loss= \\alpha * Loss_i + \\beta * Loss_s$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Loss_i$ is the intent detection task loss \n",
    "- $Loss_s$ is slot filling task loss\n",
    "- $\\alpha + \\beta = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7069301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=2, pad_index=0):\n",
    "        super(BiGRU, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.GRU(emb_size, hid_size, n_layer,dropout=0.4, bidirectional=True)    \n",
    "        self.slot_out = nn.Linear(hid_size*2, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, last_hidden = self.utt_encoder(packed_input) \n",
    "        \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        \n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb2fe1",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bac01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0005 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200fe00",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2bf062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_bigru(data, optimizer, criterion_slots, criterion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        \n",
    "        alpha=random.uniform(0, 1)\n",
    "        beta=1-alpha\n",
    "        \n",
    "        #alpha=0.3\n",
    "        #beta=0.7\n",
    "        \n",
    "        loss=max(alpha,beta) * loss_slot + min(alpha,beta) * loss_intent\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c079dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [03:04<00:00, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS\n",
      "Slot F1 0.949 +- 0.001\n",
      "Intent Acc 0.956 +- 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    ATIS_bigru = BiGRU(hid_size, ATIS_out_slot, ATIS_out_int, emb_size, ATIS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    ATIS_bigru.apply(init_weights)\n",
    "    \n",
    "    ATIS_optimizer = optim.Adam(ATIS_bigru.parameters(), lr=lr)\n",
    "    ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    ATIS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop_bigru(ATIS_train_loader, ATIS_optimizer, ATIS_criterion_slots, \n",
    "                      ATIS_criterion_intents, ATIS_bigru)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(ATIS_dev_loader, ATIS_criterion_slots, \n",
    "                                                          ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    ATIS_results_test, ATIS_intent_test, _ = eval_loop(ATIS_test_loader, ATIS_criterion_slots, \n",
    "                                         ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "    intent_acc.append(ATIS_intent_test['accuracy'])\n",
    "    slot_f1s.append(ATIS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print(\"ATIS\")\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea956ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [05:09<00:00, 61.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIPS\n",
      "Slot F1 0.891 +- 0.008\n",
      "Intent Acc 0.97 +- 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    SNIPS_bigru = BiGRU(hid_size, SNIPS_out_slot, SNIPS_out_int, emb_size, SNIPS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    SNIPS_bigru.apply(init_weights)\n",
    "    \n",
    "    SNIPS_optimizer = optim.Adam(SNIPS_bigru.parameters(), lr=lr)\n",
    "    SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    SNIPS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop_bigru(SNIPS_train_loader, SNIPS_optimizer, SNIPS_criterion_slots, \n",
    "                      SNIPS_criterion_intents, SNIPS_bigru)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(SNIPS_dev_loader, SNIPS_criterion_slots, \n",
    "                                                          SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    SNIPS_results_test, SNIPS_intent_test, _ = eval_loop(SNIPS_test_loader, SNIPS_criterion_slots, \n",
    "                                         SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "    intent_acc.append(SNIPS_intent_test['accuracy'])\n",
    "    slot_f1s.append(SNIPS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print(\"SNIPS\")\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7270a6",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3d2aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  2 = 0.5\n",
      "Accuracy at sentence lenght  3 = 0.8181818181818182\n",
      "Accuracy at sentence lenght  4 = 0.9534883720930233\n",
      "Accuracy at sentence lenght  5 = 0.9743589743589743\n",
      "Accuracy at sentence lenght  6 = 0.9624060150375939\n",
      "Accuracy at sentence lenght  7 = 0.9565217391304348\n",
      "Accuracy at sentence lenght  8 = 0.9642857142857143\n",
      "Accuracy at sentence lenght  9 = 0.9644549763033176\n",
      "Accuracy at sentence lenght  10 = 0.9707602339181286\n",
      "Accuracy at sentence lenght  11 = 0.97\n",
      "Accuracy at sentence lenght  12 = 0.9657228017883756\n",
      "Accuracy at sentence lenght  13 = 0.9656121045392022\n",
      "Accuracy at sentence lenght  14 = 0.9649805447470817\n",
      "Accuracy at sentence lenght  15 = 0.9611041405269761\n",
      "Accuracy at sentence lenght  16 = 0.9604790419161676\n",
      "Accuracy at sentence lenght  17 = 0.9581881533101045\n",
      "Accuracy at sentence lenght  18 = 0.9572254335260115\n",
      "Accuracy at sentence lenght  19 = 0.957471264367816\n",
      "Accuracy at sentence lenght  20 = 0.95662100456621\n",
      "Accuracy at sentence lenght  21 = 0.9548532731376975\n",
      "Accuracy at sentence lenght  22 = 0.954954954954955\n",
      "Accuracy at sentence lenght  23 = 0.9550056242969629\n",
      "Accuracy at sentence lenght  29 = 0.9551569506726457\n",
      "Accuracy at sentence lenght  30 = 0.9552071668533034\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(ATIS_test_loader, ATIS_criterion_slots,\n",
    "                                              ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3960a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  3 = 0.8888888888888888\n",
      "Accuracy at sentence lenght  4 = 0.8857142857142857\n",
      "Accuracy at sentence lenght  5 = 0.9397590361445783\n",
      "Accuracy at sentence lenght  6 = 0.9440993788819876\n",
      "Accuracy at sentence lenght  7 = 0.9418604651162791\n",
      "Accuracy at sentence lenght  8 = 0.9394812680115274\n",
      "Accuracy at sentence lenght  9 = 0.9483568075117371\n",
      "Accuracy at sentence lenght  10 = 0.9548254620123203\n",
      "Accuracy at sentence lenght  11 = 0.9566003616636528\n",
      "Accuracy at sentence lenght  12 = 0.9595278246205734\n",
      "Accuracy at sentence lenght  13 = 0.960127591706539\n",
      "Accuracy at sentence lenght  14 = 0.9600614439324117\n",
      "Accuracy at sentence lenght  15 = 0.960960960960961\n",
      "Accuracy at sentence lenght  16 = 0.9604105571847508\n",
      "Accuracy at sentence lenght  17 = 0.9608127721335269\n",
      "Accuracy at sentence lenght  18 = 0.9609826589595376\n",
      "Accuracy at sentence lenght  19 = 0.9610951008645533\n",
      "Accuracy at sentence lenght  20 = 0.9598278335724534\n",
      "Accuracy at sentence lenght  21 = 0.9598853868194842\n",
      "Accuracy at sentence lenght  22 = 0.9599427753934192\n",
      "Accuracy at sentence lenght  24 = 0.96\n",
      "=====================================================================================\n",
      "SNIPS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD6CAYAAACxrrxPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR1UlEQVR4nO3ce7BdZX3G8e8jES+oeMnRIgmGanBkrFVMkdZrRZ2ATlKvA6OtVpSpFYuX2sHqUMXpjPd2OqVaqlRFBfHaqLHgBaXTESQgICGiEVESFaL10tapiP76x1qxu4d99l77sIPk9fuZ2XPW5f3t9z3nvOc5a6+1105VIUna+93mVz0ASdJ8GOiS1AgDXZIaYaBLUiMMdElqhIEuSY2YGuhJTk9yfZIrltifJH+XZHuSy5McNv9hSpKmWTGgzTuBvwfevcT+o4C1/eNhwFv7rxOtXLmy1qxZM2iQkqTOxRdf/L2qWhi3b2qgV9X5SdZMaLIReHd1dyhdkOSuSQ6oqu9Met41a9awZcuWad1LkkYk+eZS++ZxDv1A4NqR9R39tnEDOT7JliRbdu3aNYeuJUm73aIXRavqtKpaV1XrFhbGvmKQJC3TPAJ9J7B6ZH1Vv02SdAuaR6BvAv6of7fLEcCPpp0/lyTN39SLoknOBB4DrEyyA/gr4LYAVfU2YDNwNLAd+Anwx3tqsJKkpQ15l8uxU/YX8MK5jUiStCzeKSpJjTDQJakRBrokNWLIrf/SrcKakz4xuO01r3uiddYtq245ZulrHv0txUDXsu0Nf2jSrxMDXQas1AgDfQ8yKCXdkgz0AW7pYPZUhqTl8F0uktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhqxV9767y3uknRTe2WgL5f/CCS1zFMuktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRgwI9yfokVyXZnuSkMfsPSnJeki8luTzJ0fMfqiRpkqmBnmQf4FTgKOBQ4Ngkhy5q9irg7Kp6CHAM8A/zHqgkabIhR+iHA9ur6uqqugE4C9i4qE0Bd+mX9we+Pb8hSpKGGBLoBwLXjqzv6LeNejXwrCQ7gM3Ai8Y9UZLjk2xJsmXXrl3LGK4kaSnzuih6LPDOqloFHA2ckeQmz11Vp1XVuqpat7CwMKeuJUkwLNB3AqtH1lf120YdB5wNUFVfAG4PrJzHACVJwwwJ9IuAtUkOTrIv3UXPTYvafAs4EiDJA+gC3XMqknQLmhroVXUjcAJwDrCN7t0sW5OckmRD3+xlwPOTXAacCTynqmpPDVqSdFMrhjSqqs10FztHt508snwl8PD5Dk2SNAvvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiEGBnmR9kquSbE9y0hJtnpHkyiRbk7xvvsOUJE2zYlqDJPsApwKPB3YAFyXZVFVXjrRZC7wCeHhV/SDJPffUgCVJ4w05Qj8c2F5VV1fVDcBZwMZFbZ4PnFpVPwCoquvnO0xJ0jRDAv1A4NqR9R39tlGHAIck+fckFyRZP68BSpKGmXrKZYbnWQs8BlgFnJ/kt6rqh6ONkhwPHA9w0EEHzalrSRIMO0LfCaweWV/Vbxu1A9hUVT+rqm8AX6UL+P+nqk6rqnVVtW5hYWG5Y5YkjTEk0C8C1iY5OMm+wDHApkVtPkp3dE6SlXSnYK6e3zAlSdNMDfSquhE4ATgH2AacXVVbk5ySZEPf7Bzg+0muBM4DXl5V399Tg5Yk3dSgc+hVtRnYvGjbySPLBby0f0iSfgW8U1SSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwYFepL1Sa5Ksj3JSRPaPTVJJVk3vyFKkoaYGuhJ9gFOBY4CDgWOTXLomHZ3Bk4ELpz3ICVJ0w05Qj8c2F5VV1fVDcBZwMYx7V4LvB74nzmOT5I00JBAPxC4dmR9R7/tl5IcBqyuqk/McWySpBnc7IuiSW4DvAV42YC2xyfZkmTLrl27bm7XkqQRQwJ9J7B6ZH1Vv223OwMPBD6X5BrgCGDTuAujVXVaVa2rqnULCwvLH7Uk6SaGBPpFwNokByfZFzgG2LR7Z1X9qKpWVtWaqloDXABsqKote2TEkqSxpgZ6Vd0InACcA2wDzq6qrUlOSbJhTw9QkjTMiiGNqmozsHnRtpOXaPuYmz8sSdKsvFNUkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqxKBAT7I+yVVJtic5acz+lya5MsnlST6T5D7zH6okaZKpgZ5kH+BU4CjgUODYJIcuavYlYF1VPQj4IPCGeQ9UkjTZkCP0w4HtVXV1Vd0AnAVsHG1QVedV1U/61QuAVfMdpiRpmiGBfiBw7cj6jn7bUo4DPjluR5Ljk2xJsmXXrl3DRylJmmquF0WTPAtYB7xx3P6qOq2q1lXVuoWFhXl2LUm/9lYMaLMTWD2yvqrf9v8keRzwSuDRVfXT+QxPkjTUkCP0i4C1SQ5Osi9wDLBptEGShwD/CGyoquvnP0xJ0jRTA72qbgROAM4BtgFnV9XWJKck2dA3eyNwJ+ADSS5NsmmJp5Mk7SFDTrlQVZuBzYu2nTyy/Lg5j0uSNCPvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiEGBnmR9kquSbE9y0pj9t0vy/n7/hUnWzH2kkqSJpgZ6kn2AU4GjgEOBY5McuqjZccAPqup+wN8Ar5/3QCVJkw05Qj8c2F5VV1fVDcBZwMZFbTYC7+qXPwgcmSTzG6YkaZpU1eQGydOA9VX1vH79D4GHVdUJI22u6Nvs6Ne/3rf53qLnOh44vl+9P3DVvL6R3krge1NbWWfd3jFG6249dcu1J/q7T1UtjNuxYs4dTVRVpwGn7annT7KlqtZZZ92tqS/r9v665bql+xtyymUnsHpkfVW/bWybJCuA/YHvz2OAkqRhhgT6RcDaJAcn2Rc4Bti0qM0m4Nn98tOAz9a0czmSpLmaesqlqm5McgJwDrAPcHpVbU1yCrClqjYB7wDOSLId+A+60P9VWO7pHOt+/er2hjFad+upW65btL+pF0UlSXsH7xSVpEYY6JLUiL0+0JPcPskXk1yWZGuS18xYv0+SLyX5+Aw11yT5cpJLk2yZoe6uST6Y5CtJtiX53QE19+/72f34cZIXD+zvJf3P5IokZya5/cC6E/uarZP6SnJ6kuv7+xB2b7t7kk8l+Vr/9W4D657e9/eLJGPf5rVE3Rv7n+flST6S5K4D617b11ya5Nwk9x5SN7LvZUkqycqB/b06yc6R3+PRQ/tL8qL+e9ya5A0D+3v/SF/XJLl0YN2Dk1ywe24nOXxg3W8n+UL/d/GxJHcZU7c6yXlJruy/lxP77RPnzIS6iXNmQt3UObMcS/U3sn/JOTM3VbVXP4AAd+qXbwtcCBwxQ/1LgfcBH5+h5hpg5TLG+i7gef3yvsBdZ6zfB/gu3Y0F09oeCHwDuEO/fjbwnAF1DwSuAO5Id9H808D9lmj7KOAw4IqRbW8ATuqXTwJeP7DuAXQ3m30OWDdDf08AVvTLr5+hv7uMLP8Z8LYhdf321XRvEvjmuHmwRH+vBv58ys9+XN3v97+D2/Xr9xw6zpH9bwZOHtjfucBR/fLRwOcG1l0EPLpffi7w2jF1BwCH9ct3Br5K93EiE+fMhLqJc2ZC3dQ5s5zHUv0NmTPzeuz1R+jV+a9+9bb9Y9CV3iSrgCcCb99Dwxvta3+6P4R3AFTVDVX1wxmf5kjg61X1zYHtVwB3SHdvwB2Bbw+oeQBwYVX9pKpuBD4PPGVcw6o6n+5dTaNGPwbiXcAfDKmrqm1VNfHO4SXqzu3HCXAB3X0SQ+p+PLK6H2PmzBLfH3SfV/QX42qm1E20RN0LgNdV1U/7NtfP0l+SAM8AzhxYV8Duo+v9GTNnlqg7BDi/X/4U8NQxdd+pqkv65f8EttEdeEycM0vVTZszE+qmzpnlmPD9wZQ5My97faDDL0+bXApcD3yqqi4cWPq3dD/kX8zYZQHnJrk43ccZDHEwsAv453SneN6eZL8Z+z2GMX+YYwdYtRN4E/At4DvAj6rq3AGlVwCPTHKPJHekO0pbPaVm1L2q6jv98neBe81Qe3M9F/jk0MZJ/jrJtcAzgZMH1mwEdlbVZcsY3wn9y/zTx52KWsIhdL+PC5N8PsnvzNjnI4HrquprA9u/GHhj/3N5E/CKgXVb+b/PeHo6U+ZMuk9kfQjdK+rBc2ZR3WAT6maaM8vp72bOmZk0EehV9fOqejDdf9rDkzxwWk2SJwHXV9XFy+jyEVV1GN0nUL4wyaMG1Kyge5n61qp6CPDfdC8vB0l3U9cG4AMD29+N7g/sYODewH5JnjWtrqq20b0MPRf4V+BS4OdDx7nouYo9fESyW5JXAjcC7x1aU1WvrKrVfc0J09r3/+D+koHhv8hbgfsCD6b7B/vmgXUrgLsDRwAvB87uj7qHOpaBBwG9FwAv6X8uL6F/RTnAc4E/TXIx3emGG5ZqmOROwIeAFy96pTRxzkyqm2SpuuXMmVn7659/uXNmZk0E+m79KYzzgPUDmj8c2JDkGrpPkHxskvcM7Gdn//V64CN0n0g5zQ5gx8irhw/SBfxQRwGXVNV1A9s/DvhGVe2qqp8BHwZ+b0hhVb2jqh5aVY8CfkB3LnCo65IcANB/vckpgnlL8hzgScAz+0CY1XsZc4pgjPvS/YO8rJ83q4BLkvzGtMKquq4/8PgF8E8MmzPQzZsP96cWv0j3anLQRbX+VNtTgPcP7Au6O74/3C9/YOg4q+orVfWEqnoo3T+Qry8xptvShd17q2p3P1PnzBJ1Uy1VN4c5M7S/Zc+Z5djrAz3Jwu6r1EnuADwe+Mq0uqp6RVWtqqo1dKcyPltVU49gk+yX5M67l+kusNzkXRBj+vsucG2S+/ebjgSunFY3YtYjrW8BRyS5Y39EdyTdOb2pktyz/3oQXSC8b4Z+Rz8G4tnAv8xQO7Mk6+lOm22oqp/MULd2ZHUjw+bMl6vqnlW1pp83O+gugn13QH8HjKw+mQFzpvdRugujJDmE7mL60E/vexzwleo/BXWgbwOP7pcfCww6VTMyZ24DvAp425g2oTvi31ZVbxnZNXHOTKibNqaxdcudM8vp7+bMmWUZcuX01vwAHgR8Cbic7o/kJlfzBzzHYxj4LhfgN4HL+sdW4JUz9PNgYEs/1o8CdxtYtx/dh53tP+P39Rq6oLoCOIP+nRID6v6N7p/NZcCRE9qdSXf64Gf9RD0OuAfwGbog+DRw94F1T+6XfwpcB5wzsG47cC3dqaFLGf9ulXF1H+p/LpcDH6O7WDa1btH+axj/Lpdx/Z0BfLnvbxNwwMC6fYH39GO9BHjs0HEC7wT+ZMbf3yOAi/vf/YXAQwfWnUj3Su6rwOvo70JfVPcIutMpl4/8vo6eNmcm1E2cMxPqps6Z5TyW6m/InJnXw1v/JakRe/0pF0lSx0CXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjfhfjeNeflBU3QEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNIPS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(SNIPS_test_loader, SNIPS_criterion_slots,\n",
    "                                              SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f939c",
   "metadata": {},
   "source": [
    "# 4) Third Model - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "465e4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4776ba6",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62d19239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "max_length=50\n",
    "train_batch_size=16\n",
    "dev_batch_size=8\n",
    "test_batch_size=8\n",
    "epochs=10\n",
    "learning_rate=0.00005\n",
    "hidden_size=768 #Features generated by Bert encoder\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME='bert-base-uncased' #bert pretrained model I want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd586b",
   "metadata": {},
   "source": [
    "### Bert tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0edfb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as transformers\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7f57764",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=\"My name is Jacopo and i would like to book a plane\"\n",
    "encodings=tokenizer.encode_plus(example,add_special_tokens=True,\n",
    "                                max_length=max_length,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               return_attention_mask=True,\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17969867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodings#Starts with CLS(101) and ends with SEP(102)\n",
    "#Contains 3 tensors: input_ids ,token_type_ids ,attention_mask , need all 3 for training\n",
    "#Bert tokenizer separates unknown words, for example Jacopo is broken into 3 separate tokens\n",
    "#Sentence is padded to max_lenght, in this case 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.convert_ids_to_tokens(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f965f",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Since BERT uses a different token embedding, I've redone the dataloader functions using the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6c7c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLang():\n",
    "    def __init__(self, intents, slots):\n",
    "        self.word2id = self.load_bert_dict()\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def load_bert_dict(self):\n",
    "        vocab={}\n",
    "        vocab_file = open('bert_vocab.txt', encoding=\"utf8\")\n",
    "        Lines = vocab_file.readlines()\n",
    "        word_id = 0\n",
    "        for line in Lines:\n",
    "            word=line.rstrip()\n",
    "            vocab[word]=word_id\n",
    "            word_id += 1\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Vocab size:\",len(self.word2id))\n",
    "        print(\"Slot size:\",len(self.slot2id)-1)\n",
    "        print(\"Number of intents\", len(self.intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12050ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    #def __init__(self, dataset, slots, intent, tokenizer, max_len):\n",
    "    def __init__(self, dataset,lang,tokenizer,max_len):\n",
    "        self.utterances = []\n",
    "        self.slots = []\n",
    "        self.intents = []\n",
    "        self.lang=lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        for x in dataset:\n",
    "            utterance=x['utterance']\n",
    "            utterance=utterance.replace(\"   \",\" \")#In SNIPS i noticed a problem in the dataset, some sentences have two or morewhitespaces\n",
    "            utterance=utterance.replace(\"  \",\" \")#between words (example \"play a tune or two from kansas city  missouri\"  between city and missouri there are two whitespace)\n",
    "                                                #since bert tokenizer wrongly recognize ' ' as a token, I remove the extra whitespace while loading\n",
    "            self.utterances.append(utterance)\n",
    "            \n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        utterance = str(self.utterances[item])\n",
    "        lang=self.lang\n",
    "        intent = self.intent_ids[item]\n",
    "        slots = self.slot_ids[item]\n",
    "        slots_len = self.max_len,\n",
    "        sent_len = len(slots)\n",
    "        for i in range(sent_len,self.max_len):\n",
    "            slots.append(PAD_TOKEN)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          utterance,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'utterance': utterance,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),#'targets': torch.tensor(target, dtype=torch.long)\n",
    "          'intent': intent,\n",
    "          'slots_len': slots_len,\n",
    "          'slots': torch.tensor(slots, dtype=torch.long)  \n",
    "        }\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d32e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_corpus = ATIS_tmp_train_raw + ATIS_test_raw \n",
    "ATIS_slots = set(sum([line['slots'].split() for line in ATIS_corpus],[]))\n",
    "ATIS_intents = set([line['intent'] for line in ATIS_corpus])\n",
    "\n",
    "ATIS_lang = BertLang(ATIS_intents, ATIS_slots)\n",
    "\n",
    "ATIS_train_dataset = BERTDataset(ATIS_train_raw, ATIS_lang, tokenizer, max_length)\n",
    "ATIS_dev_dataset = BERTDataset(ATIS_dev_raw, ATIS_lang, tokenizer, max_length)\n",
    "ATIS_test_dataset = BERTDataset(ATIS_test_raw, ATIS_lang, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8dff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPS_corpus = SNIPS_tmp_train_raw + SNIPS_test_raw \n",
    "SNIPS_slots = set(sum([line['slots'].split() for line in SNIPS_corpus],[]))\n",
    "SNIPS_intents = set([line['intent'] for line in SNIPS_corpus])\n",
    "\n",
    "SNIPS_lang = BertLang(SNIPS_intents, SNIPS_slots)\n",
    "\n",
    "SNIPS_train_dataset = BERTDataset(SNIPS_train_raw, SNIPS_lang, tokenizer, max_length)\n",
    "SNIPS_dev_dataset = BERTDataset(SNIPS_dev_raw, SNIPS_lang, tokenizer, max_length)\n",
    "SNIPS_test_dataset = BERTDataset(SNIPS_test_raw, SNIPS_lang, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19649a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ATIS_train_dataset.__getitem__(456))\n",
    "\n",
    "#input id traduce la frase con il vocab di bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0b04368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS\n",
      "Train:  4384\n",
      "Dev:  600\n",
      "Test:  896\n",
      "=====================================================================================\n",
      "SNIPS\n",
      "Train:  13088\n",
      "Dev:  704\n",
      "Test:  704\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(dataset,batch_size):\n",
    "    return DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "ATIS_train_dataloader = create_data_loader(ATIS_train_dataset, train_batch_size)\n",
    "ATIS_dev_dataloader = create_data_loader(ATIS_dev_dataset, dev_batch_size)\n",
    "ATIS_test_dataloader = create_data_loader(ATIS_test_dataset, test_batch_size)\n",
    "\n",
    "SNIPS_train_dataloader = create_data_loader(SNIPS_train_dataset, train_batch_size)\n",
    "SNIPS_dev_dataloader = create_data_loader(SNIPS_dev_dataset, dev_batch_size)\n",
    "SNIPS_test_dataloader = create_data_loader(SNIPS_test_dataset, test_batch_size)\n",
    "\n",
    "#print(len(ATIS_train_dataloader),train_batch_size)\n",
    "print(\"ATIS\")\n",
    "print(\"Train: \",len(ATIS_train_dataloader)*train_batch_size)\n",
    "print(\"Dev: \",len(ATIS_dev_dataloader)*dev_batch_size)\n",
    "print(\"Test: \",len(ATIS_test_dataloader)*test_batch_size)\n",
    "print(\"=\"*85)\n",
    "print(\"SNIPS\")\n",
    "print(\"Train: \",len(SNIPS_train_dataloader)*train_batch_size)\n",
    "print(\"Dev: \",len(SNIPS_dev_dataloader)*dev_batch_size)\n",
    "print(\"Test: \",len(SNIPS_test_dataloader)*test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d016d",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fccc8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJoint(nn.Module):\n",
    "    def __init__(self,hid_size, out_slot, out_int):\n",
    "        super(BertJoint,self).__init__()\n",
    "        self.bert_encoder= BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        #self.dropout=nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs= self.bert_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        #returns sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1] \n",
    "\n",
    "        intents = self.intent_out(pooled_output)\n",
    "        slots = self.slot_out(sequence_output)\n",
    "        \n",
    "        #Slot size: batch size, seq len, classes\n",
    "        slots = slots.permute(0,2,1) #Used for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        \n",
    "        return slots, intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd8fc4",
   "metadata": {},
   "source": [
    "### Training Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9f169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "ATIS_criterion_intents = nn.CrossEntropyLoss()\n",
    "\n",
    "SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "SNIPS_criterion_intents = nn.CrossEntropyLoss()\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "506255c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertJoint(\n",
       "  (bert_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (slot_out): Linear(in_features=768, out_features=130, bias=True)\n",
       "  (intent_out): Linear(in_features=768, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ATIS\n",
    "ATIS_model=BertJoint(hidden_size,ATIS_out_slot,ATIS_out_int)\n",
    "ATIS_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73e30ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertJoint(\n",
       "  (bert_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (slot_out): Linear(in_features=768, out_features=73, bias=True)\n",
       "  (intent_out): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SNIPS\n",
    "SNIPS_model=BertJoint(hidden_size,SNIPS_out_slot,SNIPS_out_int)\n",
    "SNIPS_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb254483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS\n",
    "ATIS_optimizer= torch.optim.AdamW(params=ATIS_model.parameters(),lr=learning_rate)\n",
    "ATIS_total_steps=len(ATIS_train_dataloader)*epochs\n",
    "ATIS_scheduler=transformers.get_linear_schedule_with_warmup(ATIS_optimizer,num_warmup_steps=0,num_training_steps=ATIS_total_steps)\n",
    "\n",
    "#SNIPS\n",
    "SNIPS_optimizer= torch.optim.AdamW(params=SNIPS_model.parameters(),lr=learning_rate)\n",
    "SNIPS_total_steps=len(SNIPS_train_dataloader)*epochs\n",
    "SNIPS_scheduler=transformers.get_linear_schedule_with_warmup(SNIPS_optimizer,num_warmup_steps=0,num_training_steps=SNIPS_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f69469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,intent_criterion,slots_criterion,optimizer,device,scheduler):\n",
    "    \n",
    "    model=model.train()\n",
    "    \n",
    "    loss_array = []\n",
    "    \n",
    "    for d in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_id=d['input_ids'].to(device)\n",
    "        attention_mask=d['attention_mask'].to(device)\n",
    "        intent_gt=d['intent'].to(device)\n",
    "        slot_gt=d['slots'].to(device)\n",
    "        \n",
    "        slot_pred,intent_pred=model(input_ids=input_id, attention_mask=attention_mask)\n",
    "        \n",
    "        loss_intent=intent_criterion(intent_pred,intent_gt)\n",
    "        \n",
    "        loss_slot=slots_criterion(slot_pred,slot_gt)\n",
    "        \n",
    "        #loss=loss_intent+loss_slot\n",
    "        \n",
    "        alpha=random.uniform(0, 1)\n",
    "        beta=1-alpha\n",
    "        \n",
    "        loss=max(alpha,beta) * loss_slot + min(alpha,beta) * loss_intent\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        optimizer.step() # Update the weights\n",
    "        scheduler.step()\n",
    "        \n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e5ce6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert(model, data_loader, intent_criterion,slots_criterion, lang,device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    y_slots = []\n",
    "    hyp_slots = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in data_loader:\n",
    "            input_ids = sample[\"input_ids\"].to(device)\n",
    "            attention_mask = sample[\"attention_mask\"].to(device)\n",
    "            ref_slots = sample[\"slots\"].to(device)\n",
    "            ref_intent = sample[\"intent\"].to(device)\n",
    "\n",
    "            slots, intents = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            \n",
    "            loss_intent = intent_criterion(intents, ref_intent)\n",
    "            loss_slot = slots_criterion(slots, ref_slots)\n",
    "            loss = loss_intent + loss_slot\n",
    "            \n",
    "            loss_array.append(loss.item())\n",
    "            \n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intent'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1) #Returns the indices of the maximum value of all elements in the input tensor.\n",
    "            ref_slots=ref_slots.tolist()\n",
    "            slots_len=len(ref_slots)\n",
    "            \n",
    "            \n",
    "            for id_seq, seq in enumerate(output_slots):#id_seq  il sample, seq  l'indice dello slot(id dello slot)\n",
    "                length = sample['slots_len'][0].data[0].item()\n",
    "                utterance = sample['utterance'][id_seq].split(\" \")\n",
    "                \n",
    "                gt_ids = sample['slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids]\n",
    "                \n",
    "                #Compute\n",
    "                tmp_ref=[]\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_ref.append((utterance[id_el], gt_slots[id_el]))\n",
    "                y_slots.append(tmp_ref)\n",
    "                \n",
    "                \n",
    "                slot_ids=seq.tolist()\n",
    "                \n",
    "                tmp_seq = []\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_seq.append((utterance[id_el], lang.id2slot[slot_ids[id_el]]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(y_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee9eb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert(model, data_loader, intent_criterion,slots_criterion, lang,device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    y_slots = []\n",
    "    hyp_slots = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in data_loader:\n",
    "            input_ids = sample[\"input_ids\"].to(device)\n",
    "            attention_mask = sample[\"attention_mask\"].to(device)\n",
    "            ref_slots = sample[\"slots\"].to(device)\n",
    "            ref_intent = sample[\"intent\"].to(device)\n",
    "\n",
    "            slots, intents = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            \n",
    "            loss_intent = intent_criterion(intents, ref_intent)\n",
    "            loss_slot = slots_criterion(slots, ref_slots)\n",
    "            loss = loss_intent + loss_slot\n",
    "            \n",
    "            loss_array.append(loss.item())\n",
    "            \n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intent'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1) #Returns the indices of the maximum value of all elements in the input tensor.\n",
    "            ref_slots=ref_slots.tolist()\n",
    "            slots_len=len(ref_slots)\n",
    "            \n",
    "            \n",
    "            for id_seq, seq in enumerate(output_slots):#id_seq  il sample, seq  l'indice dello slot(id dello slot)\n",
    "                length = sample['slots_len'][0].data[0].item()\n",
    "                utterance = sample['utterance'][id_seq].split(\" \")\n",
    "                \n",
    "                gt_ids = sample['slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids]\n",
    "                \n",
    "                #Compute\n",
    "                tmp_ref=[]\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_ref.append((utterance[id_el], gt_slots[id_el]))\n",
    "                y_slots.append(tmp_ref)\n",
    "                \n",
    "                \n",
    "                slot_ids=seq.tolist()\n",
    "                \n",
    "                tmp_seq = []\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_seq.append((utterance[id_el], lang.id2slot[slot_ids[id_el]]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(y_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "\n",
    "    return hyp_intents, ref_intents, hyp_slots, y_slots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad6713",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16870dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                       | 10.0/100 [01:05<09:49,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss 0.00985997238232122\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|                                                               | 20.0/100 [02:10<08:43,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss 0.009974544785872619\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|                                                       | 30.0/100 [03:15<07:36,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train loss 0.01001571828880123\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|                                               | 40.0/100 [04:23<06:36,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train loss 0.00975952578290454\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|                                       | 50.0/100 [05:29<05:30,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train loss 0.00982390386545283\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|                               | 60.0/100 [06:35<04:24,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train loss 0.009900510806104263\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|                       | 70.0/100 [07:41<03:17,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train loss 0.010041408776985413\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|               | 80.0/100 [08:46<02:11,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train loss 0.010235771603107343\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|        | 90.0/100 [09:52<01:05,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train loss 0.009447458408344673\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100.0/100 [10:58<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train loss 0.009577791727008638\n",
      "Dev Slot F1:  0.9661229611041405\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.923997185080929\n",
      "Test Intent Accuracy: 0.9764837625979843\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertJoint' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/3506019243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0msave_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mATIS_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0matis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8844/3326602363.py\u001b[0m in \u001b[0;36msave_bert\u001b[1;34m(model, atis)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDIR_MODELS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#orch.save(model.state_dict(), path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved in\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1208\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertJoint' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "warnings.filterwarnings('ignore')#Used to ignore a warning from hugging face tokenizer that would get repeated at every epoch, gets displayed only firstime\n",
    "\n",
    "update_tick=100/epochs\n",
    "with tqdm(total=100) as pbar:#updating tqdm manually for clearer visualization\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_epoch(\n",
    "        ATIS_model,\n",
    "        ATIS_train_dataloader,\n",
    "        ATIS_criterion_intents,\n",
    "        ATIS_criterion_slots,\n",
    "        ATIS_optimizer,\n",
    "        device,\n",
    "        ATIS_scheduler)\n",
    "\n",
    "        results_dev, intent_res, loss_dev  = eval_bert(\n",
    "        ATIS_model,\n",
    "        ATIS_dev_dataloader,\n",
    "        ATIS_criterion_intents,\n",
    "        ATIS_criterion_slots,\n",
    "        ATIS_lang,\n",
    "        device)\n",
    "\n",
    "        print(\"Epoch:\",epoch)\n",
    "        print(f'Train loss {mean(train_loss)}')\n",
    "        print('Dev Slot F1: ', results_dev['total']['f'])\n",
    "        print('Dev Intent Accuracy:', intent_res['accuracy'])\n",
    "\n",
    "        pbar.update(update_tick)#updating tqdm manually for clearer visualization\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_bert(ATIS_model,atis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19910df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                       | 10.0/100 [02:54<26:09, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss 0.4609683027768776\n",
      "Slot F1:  0.8344226579520696\n",
      "Intent Accuracy: 0.9885714285714285\n",
      "Test Slot F1:  0.8269441401971523\n",
      "Test Intent Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|                                                               | 20.0/100 [05:49<23:17, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss 0.14115965118618057\n",
      "Slot F1:  0.8783969256107603\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.8631231065822087\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|                                                       | 30.0/100 [08:45<20:27, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train loss 0.0834446423595958\n",
      "Slot F1:  0.9011049723756905\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.8948982436576526\n",
      "Test Intent Accuracy: 0.9871428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|                                               | 40.0/100 [11:47<17:46, 17.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train loss 0.054004131383968465\n",
      "Slot F1:  0.9204765863119979\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.914396887159533\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|                                       | 50.0/100 [14:43<14:45, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train loss 0.03497089363217882\n",
      "Slot F1:  0.9245647969052224\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9130675526024364\n",
      "Test Intent Accuracy: 0.9785714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|                               | 60.0/100 [17:42<11:52, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train loss 0.023503464687340107\n",
      "Slot F1:  0.921840354767184\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9227348526959421\n",
      "Test Intent Accuracy: 0.9828571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|                       | 70.0/100 [20:40<08:54, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train loss 0.014939053474400592\n",
      "Slot F1:  0.9340384080155859\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9271007234279354\n",
      "Test Intent Accuracy: 0.9814285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|               | 80.0/100 [23:35<05:54, 17.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train loss 0.010266339579179558\n",
      "Slot F1:  0.9303097345132743\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9301808066759389\n",
      "Test Intent Accuracy: 0.9871428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|        | 90.0/100 [26:30<02:56, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train loss 0.006071262783178784\n",
      "Slot F1:  0.9296788482834994\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9354479688369504\n",
      "Test Intent Accuracy: 0.9857142857142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100.0/100 [29:25<00:00, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train loss 0.0038294121280766816\n",
      "Slot F1:  0.9324847814056447\n",
      "Intent Accuracy: 0.9871428571428571\n",
      "Test Slot F1:  0.9354838709677419\n",
      "Test Intent Accuracy: 0.9857142857142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "warnings.filterwarnings('ignore')#Used to ignore a warning from hugging face tokenizer that would get repeated at every epoch, gets displayed only firstime\n",
    "\n",
    "update_tick=100/epochs\n",
    "with tqdm(total=100) as pbar:#updating tqdm manually for clearer visualization\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_epoch(\n",
    "        SNIPS_model,\n",
    "        SNIPS_train_dataloader,\n",
    "        SNIPS_criterion_intents,\n",
    "        SNIPS_criterion_slots,\n",
    "        SNIPS_optimizer,\n",
    "        device,\n",
    "        SNIPS_scheduler)\n",
    "\n",
    "\n",
    "        results_dev, intent_res, loss_dev  = eval_bert(\n",
    "        SNIPS_model,\n",
    "        SNIPS_dev_dataloader,\n",
    "        SNIPS_criterion_intents,\n",
    "        SNIPS_criterion_slots,\n",
    "        SNIPS_lang,\n",
    "        device)    \n",
    "    \n",
    "        print(\"Epoch:\",epoch)\n",
    "        print(f'Train loss {mean(train_loss)}')\n",
    "        print('Slot F1: ',results_dev['total']['f'])\n",
    "        print('Intent Accuracy:', intent_res['accuracy'])\n",
    "\n",
    "        pbar.update(update_tick)#updating tqdm manually for clearer visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ca8e3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce968b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS\n",
      "Slot F1:  0.923997185080929\n",
      "Intent Accuracy: 0.9764837625979843\n",
      "\n",
      "SNIPS\n",
      "Slot F1:  0.9354838709677419\n",
      "Intent Accuracy: 0.9857142857142858\n"
     ]
    }
   ],
   "source": [
    "ATIS_results_test, ATIS_intent_test, _ = eval_bert(ATIS_model, ATIS_test_dataloader, ATIS_criterion_intents, \n",
    "                                                   ATIS_criterion_slots, ATIS_lang,device)\n",
    "print(\"ATIS\")\n",
    "print('Slot F1: ',ATIS_results_test['total']['f'])\n",
    "print('Intent Accuracy:', ATIS_intent_test['accuracy'])\n",
    "\n",
    "\n",
    "SNIPS_results_test, SNIPS_intent_test, _ = eval_bert(SNIPS_model, SNIPS_test_dataloader, SNIPS_criterion_intents, \n",
    "                                                   SNIPS_criterion_slots, SNIPS_lang,device)\n",
    "\n",
    "print()\n",
    "print(\"SNIPS\")\n",
    "print('Slot F1: ',SNIPS_results_test['total']['f'])\n",
    "print('Intent Accuracy:', SNIPS_intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b76b9",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5dc83b",
   "metadata": {},
   "source": [
    "Guarda i worse performing slots, secondo me bert fa fatica a fare slot filling perch non ho usato un custom vocab, ipotesi verificata se i worse performing label sono nomi di luoghi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e213da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  2 = 0.5\n",
      "Accuracy at sentence lenght  3 = 0.8181818181818182\n",
      "Accuracy at sentence lenght  4 = 0.9534883720930233\n",
      "Accuracy at sentence lenght  5 = 0.9743589743589743\n",
      "Accuracy at sentence lenght  6 = 0.9849624060150376\n",
      "Accuracy at sentence lenght  7 = 0.9806763285024155\n",
      "Accuracy at sentence lenght  8 = 0.9837662337662337\n",
      "Accuracy at sentence lenght  9 = 0.9834123222748815\n",
      "Accuracy at sentence lenght  10 = 0.98635477582846\n",
      "Accuracy at sentence lenght  11 = 0.9866666666666667\n",
      "Accuracy at sentence lenght  12 = 0.9865871833084948\n",
      "Accuracy at sentence lenght  13 = 0.9862448418156808\n",
      "Accuracy at sentence lenght  14 = 0.9844357976653697\n",
      "Accuracy at sentence lenght  15 = 0.9824341279799247\n",
      "Accuracy at sentence lenght  16 = 0.9832335329341317\n",
      "Accuracy at sentence lenght  17 = 0.9802555168408827\n",
      "Accuracy at sentence lenght  18 = 0.9791907514450867\n",
      "Accuracy at sentence lenght  19 = 0.9793103448275862\n",
      "Accuracy at sentence lenght  20 = 0.978310502283105\n",
      "Accuracy at sentence lenght  21 = 0.9762979683972912\n",
      "Accuracy at sentence lenght  22 = 0.9763513513513513\n",
      "Accuracy at sentence lenght  23 = 0.9763779527559056\n",
      "Accuracy at sentence lenght  29 = 0.976457399103139\n",
      "Accuracy at sentence lenght  30 = 0.9764837625979843\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict_bert(ATIS_model, ATIS_test_dataloader, ATIS_criterion_intents, \n",
    "                                                   ATIS_criterion_slots, ATIS_lang,device)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5090a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  3 = 0.8888888888888888\n",
      "Accuracy at sentence lenght  4 = 0.8857142857142857\n",
      "Accuracy at sentence lenght  5 = 0.9518072289156626\n",
      "Accuracy at sentence lenght  6 = 0.9627329192546584\n",
      "Accuracy at sentence lenght  7 = 0.9689922480620154\n",
      "Accuracy at sentence lenght  8 = 0.9769452449567724\n",
      "Accuracy at sentence lenght  9 = 0.9765258215962441\n",
      "Accuracy at sentence lenght  10 = 0.9794661190965093\n",
      "Accuracy at sentence lenght  11 = 0.9819168173598554\n",
      "Accuracy at sentence lenght  12 = 0.9831365935919055\n",
      "Accuracy at sentence lenght  13 = 0.9840510366826156\n",
      "Accuracy at sentence lenght  14 = 0.9846390168970814\n",
      "Accuracy at sentence lenght  15 = 0.984984984984985\n",
      "Accuracy at sentence lenght  16 = 0.9853372434017595\n",
      "Accuracy at sentence lenght  17 = 0.9854862119013063\n",
      "Accuracy at sentence lenght  18 = 0.9855491329479769\n",
      "Accuracy at sentence lenght  19 = 0.9855907780979827\n",
      "Accuracy at sentence lenght  20 = 0.9856527977044476\n",
      "Accuracy at sentence lenght  21 = 0.9856733524355301\n",
      "Accuracy at sentence lenght  22 = 0.9856938483547926\n",
      "Accuracy at sentence lenght  24 = 0.9857142857142858\n",
      "=====================================================================================\n",
      "SNIPS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNIPS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict_bert(SNIPS_model, SNIPS_test_dataloader, SNIPS_criterion_intents, \n",
    "                                                   SNIPS_criterion_slots, SNIPS_lang,device)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14002ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
