{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20368ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import warnings\n",
    "from statistics import mean\n",
    "\n",
    "PAD_TOKEN=0\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be059ede",
   "metadata": {},
   "source": [
    "# 1) Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de211b5",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b187b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb03e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_tmp_train_raw = load_data(os.path.join('data','ATIS','train.json'))\n",
    "ATIS_test_raw = load_data(os.path.join('data','ATIS','test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae958228",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPS_train_raw = load_data(os.path.join('data','SNIPS','train.json'))\n",
    "SNIPS_dev_raw= load_data(os.path.join('data','SNIPS','valid.json'))\n",
    "SNIPS_test_raw = load_data(os.path.join('data','SNIPS','test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0baee05",
   "metadata": {},
   "source": [
    "### Create development set\n",
    "\n",
    "As suggested in class, we create a development set used for hyperparameter tuning using a stratification strategy in order to preserve the intents data distributions.\n",
    "Since SNIPS dataset already comes with a validation set, the development set for this dataset is not created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425cb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
    "# on the training set which is around 11% \n",
    "portion = round(((len(ATIS_tmp_train_raw) + len(ATIS_test_raw)) * 0.10)/(len(ATIS_tmp_train_raw)),2)\n",
    "\n",
    "intents = [x['intent'] for x in ATIS_tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)#For each class count the appearances\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # Some intents have only one instance, we put them in training\n",
    "        X.append(ATIS_tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(ATIS_tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "ATIS_X_train, ATIS_X_dev, ATIS_y_train, ATIS_y_dev = train_test_split(X, Y, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "ATIS_X_train.extend(mini_Train)\n",
    "ATIS_train_raw = ATIS_X_train\n",
    "ATIS_dev_raw = ATIS_X_dev\n",
    "\n",
    "ATIS_y_test = [x['intent'] for x in ATIS_test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(ATIS_y_train),3)*100 for k, v in sorted(Counter(ATIS_y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(ATIS_y_dev),3)*100 for k, v in sorted(Counter(ATIS_y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(ATIS_y_test),3)*100 for k, v in sorted(Counter(ATIS_y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(ATIS_train_raw))\n",
    "print('DEV size:', len(ATIS_dev_raw))\n",
    "print('TEST size:', len(ATIS_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2857cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#SNIPS\\n# Firt we get the 10% of dataset, then we compute the percentage of these examples \\n# on the training set which is around 11% \\nportion = round(((len(SNIPS_tmp_train_raw) + len(SNIPS_test_raw)) * 0.10)/(len(SNIPS_tmp_train_raw)),2)\\n\\nintents = [x['intent'] for x in SNIPS_tmp_train_raw] # We stratify on intents\\ncount_y = Counter(intents)#For each class count the appearances\\n\\nX = []\\nY = []\\nmini_Train = []\\n\\nfor id_y, y in enumerate(intents):\\n    if count_y[y] > 1: # Some intents have only one instance, we put them in training\\n        X.append(SNIPS_tmp_train_raw[id_y])\\n        Y.append(y)\\n    else:\\n        mini_Train.append(SNIPS_tmp_train_raw[id_y])\\n# Random Stratify\\nSNIPS_X_train, SNIPS_X_dev, SNIPS_y_train, SNIPS_y_dev = train_test_split(X, Y, test_size=portion, \\n                                                    random_state=42, \\n                                                    shuffle=True,\\n                                                    stratify=Y)\\nSNIPS_X_train.extend(mini_Train)\\nSNIPS_train_raw = SNIPS_X_train\\nSNIPS_dev_raw = SNIPS_X_dev\\n\\nSNIPS_y_test = [x['intent'] for x in SNIPS_test_raw]\\n\\n# Intent distribution\\nprint('Train:')\\npprint({k:round(v/len(SNIPS_y_train),3)*100 for k, v in sorted(Counter(SNIPS_y_train).items())})\\nprint('Dev:'), \\npprint({k:round(v/len(SNIPS_y_dev),3)*100 for k, v in sorted(Counter(SNIPS_y_dev).items())})\\nprint('Test:') \\npprint({k:round(v/len(SNIPS_y_test),3)*100 for k, v in sorted(Counter(SNIPS_y_test).items())})\\nprint('='*89)\\n# Dataset size\\nprint('TRAIN size:', len(SNIPS_train_raw))\\nprint('DEV size:', len(SNIPS_dev_raw))\\nprint('TEST size:', len(SNIPS_test_raw))\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#SNIPS\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples \n",
    "# on the training set which is around 11% \n",
    "portion = round(((len(SNIPS_tmp_train_raw) + len(SNIPS_test_raw)) * 0.10)/(len(SNIPS_tmp_train_raw)),2)\n",
    "\n",
    "intents = [x['intent'] for x in SNIPS_tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)#For each class count the appearances\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # Some intents have only one instance, we put them in training\n",
    "        X.append(SNIPS_tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(SNIPS_tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "SNIPS_X_train, SNIPS_X_dev, SNIPS_y_train, SNIPS_y_dev = train_test_split(X, Y, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "SNIPS_X_train.extend(mini_Train)\n",
    "SNIPS_train_raw = SNIPS_X_train\n",
    "SNIPS_dev_raw = SNIPS_X_dev\n",
    "\n",
    "SNIPS_y_test = [x['intent'] for x in SNIPS_test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(SNIPS_y_train),3)*100 for k, v in sorted(Counter(SNIPS_y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(SNIPS_y_dev),3)*100 for k, v in sorted(Counter(SNIPS_y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(SNIPS_y_test),3)*100 for k, v in sorted(Counter(SNIPS_y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(SNIPS_train_raw))\n",
    "print('DEV size:', len(SNIPS_dev_raw))\n",
    "print('TEST size:', len(SNIPS_test_raw))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8683d",
   "metadata": {},
   "source": [
    "### Lang Class\n",
    "\n",
    "Class handling conversion of words/slots/intents to ids and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33348ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8868bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words: 861\n",
      "# Slots: 129\n",
      "# Intents: 26\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "ATIS_words = sum([x['utterance'].split() for x in ATIS_train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "ATIS_corpus = ATIS_train_raw + ATIS_dev_raw + ATIS_test_raw # We do not want unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "ATIS_slots = set(sum([line['slots'].split() for line in ATIS_corpus],[])) # the type of slots\n",
    "ATIS_intents = set([line['intent'] for line in ATIS_corpus]) #all the intent types\n",
    "\n",
    "ATIS_lang = Lang(ATIS_words, ATIS_intents, ATIS_slots, cutoff=0)\n",
    "\n",
    "print('# Words:', len(ATIS_lang.word2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(ATIS_lang.slot2id)-1)\n",
    "print('# Intents:', len(ATIS_lang.intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4150022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words: 11417\n",
      "# Slots: 72\n",
      "# Intents: 7\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "SNIPS_words = sum([x['utterance'].split() for x in SNIPS_train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "SNIPS_corpus = SNIPS_train_raw + SNIPS_dev_raw + SNIPS_test_raw # We do not want unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "SNIPS_slots = set(sum([line['slots'].split() for line in SNIPS_corpus],[])) # the type of slots\n",
    "SNIPS_intents = set([line['intent'] for line in SNIPS_corpus]) #all the intent types\n",
    "\n",
    "SNIPS_lang = Lang(SNIPS_words, SNIPS_intents, SNIPS_slots, cutoff=0)\n",
    "\n",
    "print('# Words:', len(SNIPS_lang.word2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(SNIPS_lang.slot2id)-1)\n",
    "print('# Intents:', len(SNIPS_lang.intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b7ad5",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e135b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c2ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS_ dataset\n",
    "ATIS_train_dataset = IntentsAndSlots(ATIS_train_raw, ATIS_lang)\n",
    "ATIS_dev_dataset = IntentsAndSlots(ATIS_dev_raw, ATIS_lang)\n",
    "ATIS_test_dataset = IntentsAndSlots(ATIS_test_raw, ATIS_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae96461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNIPS_ dataset\n",
    "SNIPS_train_dataset = IntentsAndSlots(SNIPS_train_raw, SNIPS_lang)\n",
    "SNIPS_dev_dataset = IntentsAndSlots(SNIPS_dev_raw, SNIPS_lang)\n",
    "SNIPS_test_dataset = IntentsAndSlots(SNIPS_test_raw, SNIPS_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f8a01",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8173eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our seleceted device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49afca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS_ Dataloader instantiation\n",
    "ATIS_train_loader = DataLoader(ATIS_train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "ATIS_dev_loader = DataLoader(ATIS_dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "ATIS_test_loader = DataLoader(ATIS_test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf30aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNIPS_ Dataloader instantiation\n",
    "SNIPS_train_loader = DataLoader(SNIPS_train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "SNIPS_dev_loader = DataLoader(SNIPS_dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "SNIPS_test_loader = DataLoader(SNIPS_test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3b3a5",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "Function to randomly initialize the weights of Neural Networks in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd82682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1623406",
   "metadata": {},
   "source": [
    "### Evaluation Loop and Predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbb099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    #tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]) if elem != 0 else (utterance[id_el], 'O'))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        \"\"\"print(len(ref_slots))\n",
    "        print(len(hyp_slots))\n",
    "        print(len(ref_slots[0]))\n",
    "        print(len(hyp_slots[0]))\"\"\"\n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9ce7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    return hyp_intents,ref_intents,hyp_slots,ref_slots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e37b78",
   "metadata": {},
   "source": [
    "### Error Analysis Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29f5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intent accuracy by sentence lenght, provides bar plot with intent accuracy per sentence lenght, useful to detect potential deficits in long sentences\n",
    "def intent_acc_lenght(intent_hyp,intent_gt, ATIS=True):\n",
    "    dataset=\"\"\n",
    "    if(ATIS):\n",
    "        dataset=\"ATIS\"\n",
    "    else:\n",
    "        dataset=\"SNIPS\"\n",
    "    test_results=[]\n",
    "    for i in range(0,50):\n",
    "        test_results.append([])\n",
    "    for i in range(0,len(intent_hyp)):\n",
    "        sent_lenght=len(slot_gt[i])\n",
    "        #print(sent_lenght)\n",
    "        test_results[sent_lenght].append((intent_hyp[i],intent_gt[i]))\n",
    "\n",
    "    #Measure accuracy at each sentence lenght\n",
    "    pred=[]\n",
    "    true=[]\n",
    "    sent_length=[]\n",
    "    accuracy_values=[]\n",
    "    accuracy_lenght=[]\n",
    "    for index in range(0,50):\n",
    "        if(len(test_results[index])!=0):\n",
    "            for (hyp, gt) in test_results[index]:\n",
    "                pred.append(hyp)\n",
    "                true.append(gt)\n",
    "            print(\"Accuracy at sentence lenght \",index,\"=\",intent_accuracy(pred, true))\n",
    "            sent_length.append(index)\n",
    "            accuracy_values.append(intent_accuracy(pred, true))\n",
    "\n",
    "    #Plot results\n",
    "    print(\"=\"*85)\n",
    "    print(dataset,\" Intent Accuracy by sentence length\")\n",
    "    plt.xticks(sent_length)\n",
    "\n",
    "    plt.bar(sent_length,accuracy_values,width=0.8)\n",
    "    plt.show()\n",
    "    #Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea4e83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_confusion_matrix():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_intent_acc():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_intent_acc():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_slots():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_slots():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844d23c",
   "metadata": {},
   "source": [
    "# 2) Baseline Model\n",
    "\n",
    "As a baseline model i took the Neural Network that was presented during lab experience #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "560cad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False)    \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee891591",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bf7c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690959ae",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a00ab76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0174c",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Multiple runs are made to compute accuracy and f1 mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881fa718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [05:28<00:00, 65.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1 0.919 +- 0.005\n",
      "Intent Acc 0.937 +- 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    ATIS_model = ModelIAS(hid_size, ATIS_out_slot, ATIS_out_int, emb_size, ATIS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    ATIS_model.apply(init_weights)\n",
    "    \n",
    "    ATIS_optimizer = optim.Adam(ATIS_model.parameters(), lr=lr)\n",
    "    ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    ATIS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop(ATIS_train_loader, ATIS_optimizer, ATIS_criterion_slots, \n",
    "                      ATIS_criterion_intents, ATIS_model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(ATIS_dev_loader, ATIS_criterion_slots, \n",
    "                                                          ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    ATIS_results_test, ATIS_intent_test, _ = eval_loop(ATIS_test_loader, ATIS_criterion_slots, \n",
    "                                         ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "    intent_acc.append(ATIS_intent_test['accuracy'])\n",
    "    slot_f1s.append(ATIS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83653410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [08:16<00:00, 99.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1 0.803 +- 0.01\n",
      "Intent Acc 0.966 +- 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    SNIPS_model = ModelIAS(hid_size, SNIPS_out_slot, SNIPS_out_int, emb_size, SNIPS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    SNIPS_model.apply(init_weights)\n",
    "    \n",
    "    SNIPS_optimizer = optim.Adam(SNIPS_model.parameters(), lr=lr)\n",
    "    SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    SNIPS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop(SNIPS_train_loader, SNIPS_optimizer, SNIPS_criterion_slots, \n",
    "                      SNIPS_criterion_intents, SNIPS_model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(SNIPS_dev_loader, SNIPS_criterion_slots, \n",
    "                                                          SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    SNIPS_results_test, SNIPS_intent_test, _ = eval_loop(SNIPS_test_loader, SNIPS_criterion_slots, \n",
    "                                         SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "    intent_acc.append(SNIPS_intent_test['accuracy'])\n",
    "    slot_f1s.append(SNIPS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147e644",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be66f7f",
   "metadata": {},
   "source": [
    "### Intent Detection Accuracy by Utterance length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "058ca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_accuracy(pred,gt):\n",
    "    lenght=len(pred)\n",
    "    correct=0\n",
    "    for i in range(0,lenght):\n",
    "        if(pred[i]==gt[i]):\n",
    "            correct+=1\n",
    "    return correct/lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c4ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  2 = 0.5\n",
      "Accuracy at sentence lenght  3 = 0.8181818181818182\n",
      "Accuracy at sentence lenght  4 = 0.9534883720930233\n",
      "Accuracy at sentence lenght  5 = 0.9743589743589743\n",
      "Accuracy at sentence lenght  6 = 0.9323308270676691\n",
      "Accuracy at sentence lenght  7 = 0.9227053140096618\n",
      "Accuracy at sentence lenght  8 = 0.935064935064935\n",
      "Accuracy at sentence lenght  9 = 0.9407582938388626\n",
      "Accuracy at sentence lenght  10 = 0.9512670565302144\n",
      "Accuracy at sentence lenght  11 = 0.9533333333333334\n",
      "Accuracy at sentence lenght  12 = 0.9493293591654247\n",
      "Accuracy at sentence lenght  13 = 0.9504814305364512\n",
      "Accuracy at sentence lenght  14 = 0.9481193255512321\n",
      "Accuracy at sentence lenght  15 = 0.944792973651192\n",
      "Accuracy at sentence lenght  16 = 0.9449101796407186\n",
      "Accuracy at sentence lenght  17 = 0.943089430894309\n",
      "Accuracy at sentence lenght  18 = 0.9421965317919075\n",
      "Accuracy at sentence lenght  19 = 0.9425287356321839\n",
      "Accuracy at sentence lenght  20 = 0.9406392694063926\n",
      "Accuracy at sentence lenght  21 = 0.9390519187358917\n",
      "Accuracy at sentence lenght  22 = 0.9391891891891891\n",
      "Accuracy at sentence lenght  23 = 0.9392575928008999\n",
      "Accuracy at sentence lenght  29 = 0.9394618834080718\n",
      "Accuracy at sentence lenght  30 = 0.9395296752519597\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(ATIS_test_loader, ATIS_criterion_slots,\n",
    "                                              ATIS_criterion_intents, ATIS_model, ATIS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "119df065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  3 = 1.0\n",
      "Accuracy at sentence lenght  4 = 0.9142857142857143\n",
      "Accuracy at sentence lenght  5 = 0.963855421686747\n",
      "Accuracy at sentence lenght  6 = 0.9503105590062112\n",
      "Accuracy at sentence lenght  7 = 0.9612403100775194\n",
      "Accuracy at sentence lenght  8 = 0.9567723342939481\n",
      "Accuracy at sentence lenght  9 = 0.9624413145539906\n",
      "Accuracy at sentence lenght  10 = 0.9650924024640657\n",
      "Accuracy at sentence lenght  11 = 0.9656419529837251\n",
      "Accuracy at sentence lenght  12 = 0.9679595278246206\n",
      "Accuracy at sentence lenght  13 = 0.9681020733652312\n",
      "Accuracy at sentence lenght  14 = 0.9662058371735791\n",
      "Accuracy at sentence lenght  15 = 0.9669669669669669\n",
      "Accuracy at sentence lenght  16 = 0.967741935483871\n",
      "Accuracy at sentence lenght  17 = 0.9666182873730044\n",
      "Accuracy at sentence lenght  18 = 0.9667630057803468\n",
      "Accuracy at sentence lenght  19 = 0.9668587896253602\n",
      "Accuracy at sentence lenght  20 = 0.9670014347202296\n",
      "Accuracy at sentence lenght  21 = 0.9670487106017192\n",
      "Accuracy at sentence lenght  22 = 0.9670958512160229\n",
      "Accuracy at sentence lenght  24 = 0.9671428571428572\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNIPS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(SNIPS_test_loader, SNIPS_criterion_slots,\n",
    "                                              SNIPS_criterion_intents, SNIPS_model, SNIPS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebcd6b",
   "metadata": {},
   "source": [
    "### Slot F1 by utterance lenght\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0087c2",
   "metadata": {},
   "source": [
    "# 3) Second Model - Bi-Directional Encoder with GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb44b74",
   "metadata": {},
   "source": [
    "The idea of this second model is to take the architecture of the baseline and apply improvements.\n",
    "The overall structure is still the same, with a joint encoder used for both Slot-Filling and Intent-Detection task and apply two separate classifiers at the end, one for each task.\n",
    "\n",
    "However, the 1-layer, monodirectional LSTM has been replaced by a bi-directional 2 layer GRU.\n",
    "The structure of the GRU implies a reduction in trainable parameters compared to LSTM, speeding up the training process and reducing the risk of overfitting.\n",
    "\n",
    "Thanks to the speeding up in training time, a layer was added to the encoder that resulted in an increase in accuracy.\n",
    "Deeper architecture with more layers were tried but were much slower in training and did not produce improvements during the evaluation compared to the 2 layer model.\n",
    "\n",
    "Also, a change in the loss computation is made:\n",
    "\n",
    "$Loss= \\alpha * Loss_i + \\beta * Loss_s$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Loss_i$ is the intent detection task loss \n",
    "- $Loss_s$ is slot filling task loss\n",
    "- $\\alpha + \\beta = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7069301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=2, pad_index=0):\n",
    "        super(BiGRU, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.GRU(emb_size, hid_size, n_layer,dropout=0.4, bidirectional=True)    \n",
    "        self.slot_out = nn.Linear(hid_size*2, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "        \n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        \n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, last_hidden = self.utt_encoder(packed_input) \n",
    "        \n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        \n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: seq_len, batch size, calsses \n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb2fe1",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bac01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0005 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200fe00",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2bf062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_bigru(data, optimizer, criterion_slots, criterion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        \n",
    "        alpha=random.uniform(0, 1)\n",
    "        beta=1-alpha\n",
    "        \n",
    "        #alpha=0.3\n",
    "        #beta=0.7\n",
    "        \n",
    "        loss=max(alpha,beta) * loss_slot + min(alpha,beta) * loss_intent\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c079dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:04<00:00, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS\n",
      "Slot F1 0.949 +- 0.001\n",
      "Intent Acc 0.956 +- 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    ATIS_bigru = BiGRU(hid_size, ATIS_out_slot, ATIS_out_int, emb_size, ATIS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    ATIS_bigru.apply(init_weights)\n",
    "    \n",
    "    ATIS_optimizer = optim.Adam(ATIS_bigru.parameters(), lr=lr)\n",
    "    ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    ATIS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop_bigru(ATIS_train_loader, ATIS_optimizer, ATIS_criterion_slots, \n",
    "                      ATIS_criterion_intents, ATIS_bigru)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(ATIS_dev_loader, ATIS_criterion_slots, \n",
    "                                                          ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    ATIS_results_test, ATIS_intent_test, _ = eval_loop(ATIS_test_loader, ATIS_criterion_slots, \n",
    "                                         ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "    intent_acc.append(ATIS_intent_test['accuracy'])\n",
    "    slot_f1s.append(ATIS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print(\"ATIS\")\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea956ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [05:09<00:00, 61.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNIPS\n",
      "Slot F1 0.891 +- 0.008\n",
      "Intent Acc 0.97 +- 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "n_epochs = 200\n",
    "early_stopping=3\n",
    "patience = early_stopping\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "\n",
    "runs = 5\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    SNIPS_bigru = BiGRU(hid_size, SNIPS_out_slot, SNIPS_out_int, emb_size, SNIPS_vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    SNIPS_bigru.apply(init_weights)\n",
    "    \n",
    "    SNIPS_optimizer = optim.Adam(SNIPS_bigru.parameters(), lr=lr)\n",
    "    SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    SNIPS_criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token\n",
    "    \n",
    "    n_epochs = 200\n",
    "    early_stopping=3\n",
    "    patience = early_stopping\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "\n",
    "    for x in range(1,n_epochs):\n",
    "        \n",
    "        loss = train_loop_bigru(SNIPS_train_loader, SNIPS_optimizer, SNIPS_criterion_slots, \n",
    "                      SNIPS_criterion_intents, SNIPS_bigru)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev=\"\"\n",
    "            results_dev, intent_res, loss_dev = eval_loop(SNIPS_dev_loader, SNIPS_criterion_slots, \n",
    "                                                          SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stoping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    SNIPS_results_test, SNIPS_intent_test, _ = eval_loop(SNIPS_test_loader, SNIPS_criterion_slots, \n",
    "                                         SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "    intent_acc.append(SNIPS_intent_test['accuracy'])\n",
    "    slot_f1s.append(SNIPS_results_test['total']['f'])\n",
    "\n",
    "\n",
    "\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print(\"SNIPS\")\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7270a6",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3d2aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  2 = 0.5\n",
      "Accuracy at sentence lenght  3 = 0.8181818181818182\n",
      "Accuracy at sentence lenght  4 = 0.9534883720930233\n",
      "Accuracy at sentence lenght  5 = 0.9743589743589743\n",
      "Accuracy at sentence lenght  6 = 0.9624060150375939\n",
      "Accuracy at sentence lenght  7 = 0.9565217391304348\n",
      "Accuracy at sentence lenght  8 = 0.9642857142857143\n",
      "Accuracy at sentence lenght  9 = 0.9644549763033176\n",
      "Accuracy at sentence lenght  10 = 0.9707602339181286\n",
      "Accuracy at sentence lenght  11 = 0.97\n",
      "Accuracy at sentence lenght  12 = 0.9657228017883756\n",
      "Accuracy at sentence lenght  13 = 0.9656121045392022\n",
      "Accuracy at sentence lenght  14 = 0.9649805447470817\n",
      "Accuracy at sentence lenght  15 = 0.9611041405269761\n",
      "Accuracy at sentence lenght  16 = 0.9604790419161676\n",
      "Accuracy at sentence lenght  17 = 0.9581881533101045\n",
      "Accuracy at sentence lenght  18 = 0.9572254335260115\n",
      "Accuracy at sentence lenght  19 = 0.957471264367816\n",
      "Accuracy at sentence lenght  20 = 0.95662100456621\n",
      "Accuracy at sentence lenght  21 = 0.9548532731376975\n",
      "Accuracy at sentence lenght  22 = 0.954954954954955\n",
      "Accuracy at sentence lenght  23 = 0.9550056242969629\n",
      "Accuracy at sentence lenght  29 = 0.9551569506726457\n",
      "Accuracy at sentence lenght  30 = 0.9552071668533034\n",
      "=====================================================================================\n",
      "ATIS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(ATIS_test_loader, ATIS_criterion_slots,\n",
    "                                              ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3960a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at sentence lenght  3 = 0.8888888888888888\n",
      "Accuracy at sentence lenght  4 = 0.8857142857142857\n",
      "Accuracy at sentence lenght  5 = 0.9397590361445783\n",
      "Accuracy at sentence lenght  6 = 0.9440993788819876\n",
      "Accuracy at sentence lenght  7 = 0.9418604651162791\n",
      "Accuracy at sentence lenght  8 = 0.9394812680115274\n",
      "Accuracy at sentence lenght  9 = 0.9483568075117371\n",
      "Accuracy at sentence lenght  10 = 0.9548254620123203\n",
      "Accuracy at sentence lenght  11 = 0.9566003616636528\n",
      "Accuracy at sentence lenght  12 = 0.9595278246205734\n",
      "Accuracy at sentence lenght  13 = 0.960127591706539\n",
      "Accuracy at sentence lenght  14 = 0.9600614439324117\n",
      "Accuracy at sentence lenght  15 = 0.960960960960961\n",
      "Accuracy at sentence lenght  16 = 0.9604105571847508\n",
      "Accuracy at sentence lenght  17 = 0.9608127721335269\n",
      "Accuracy at sentence lenght  18 = 0.9609826589595376\n",
      "Accuracy at sentence lenght  19 = 0.9610951008645533\n",
      "Accuracy at sentence lenght  20 = 0.9598278335724534\n",
      "Accuracy at sentence lenght  21 = 0.9598853868194842\n",
      "Accuracy at sentence lenght  22 = 0.9599427753934192\n",
      "Accuracy at sentence lenght  24 = 0.96\n",
      "=====================================================================================\n",
      "SNIPS  Intent Accuracy by sentence length\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNIPS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(SNIPS_test_loader, SNIPS_criterion_slots,\n",
    "                                              SNIPS_criterion_intents, SNIPS_bigru, SNIPS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f939c",
   "metadata": {},
   "source": [
    "# 4) Third Model - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "465e4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4776ba6",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d19239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "max_length=50\n",
    "train_batch_size=16\n",
    "dev_batch_size=8\n",
    "test_batch_size=8\n",
    "epochs=10\n",
    "learning_rate=0.00005\n",
    "hidden_size=768 #Features generated by Bert encoder\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME='bert-base-uncased' #bert pretrained model I want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd586b",
   "metadata": {},
   "source": [
    "### Bert tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0edfb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as transformers\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7f57764",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=\"My name is Jacopo and i would like to book a plane\"\n",
    "encodings=tokenizer.encode_plus(example,add_special_tokens=True,\n",
    "                                max_length=max_length,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               return_attention_mask=True,\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17969867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodings#Starts with CLS(101) and ends with SEP(102)\n",
    "#Contains 3 tensors: input_ids ,token_type_ids ,attention_mask , need all 3 for training\n",
    "#Bert tokenizer separates unknown words, for example Jacopo is broken into 3 separate tokens\n",
    "#Sentence is padded to max_lenght, in this case 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "981cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.convert_ids_to_tokens(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef60cb",
   "metadata": {},
   "source": [
    "### Get dataset max sentence lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "802abfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4978\n",
      "893\n",
      "46\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "#Compute max lenght for padding size\n",
    "def get_max_length(train,test):\n",
    "    max_length=0\n",
    "    for sent in train:\n",
    "        current=len(sent['utterance'].split(\" \"))\n",
    "        if current>max_length:\n",
    "            max_length=current\n",
    "    for sent in test:\n",
    "        current=len(sent['utterance'].split(\" \"))\n",
    "        if current>max_length:\n",
    "            max_length=current\n",
    "    return max_length\n",
    "\n",
    "ATIS_tmp_train_raw = load_data(os.path.join('ATIS','train.json'))\n",
    "ATIS_test_raw = load_data(os.path.join('ATIS','test.json'))\n",
    "print(len(ATIS_tmp_train_raw))\n",
    "print(len(ATIS_test_raw))\n",
    "\n",
    "print(get_max_length(ATIS_tmp_train_raw,ATIS_test_raw))\n",
    "\n",
    "SNIPS_tmp_train_raw = load_data(os.path.join('SNIPS','train.json'))\n",
    "SNIPS_test_raw = load_data(os.path.join('SNIPS','test.json'))\n",
    "\n",
    "print(get_max_length(SNIPS_tmp_train_raw,SNIPS_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f965f",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Since BERT uses a different token embedding, I've redone the dataloader functions using the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6c7c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLang():\n",
    "    def __init__(self, intents, slots):\n",
    "        self.word2id = self.load_bert_dict()\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def load_bert_dict(self):\n",
    "        vocab={}\n",
    "        vocab_file = open('bert_vocab.txt', encoding=\"utf8\")\n",
    "        Lines = vocab_file.readlines()\n",
    "        word_id = 0\n",
    "        for line in Lines:\n",
    "            word=line.rstrip()\n",
    "            vocab[word]=word_id\n",
    "            word_id += 1\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Vocab size:\",len(self.word2id))\n",
    "        print(\"Slot size:\",len(self.slot2id)-1)\n",
    "        print(\"Number of intents\", len(self.intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12050ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    #def __init__(self, dataset, slots, intent, tokenizer, max_len):\n",
    "    def __init__(self, dataset,lang,tokenizer,max_len):\n",
    "        self.utterances = []\n",
    "        self.slots = []\n",
    "        self.intents = []\n",
    "        self.lang=lang\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        for x in dataset:\n",
    "            utterance=x['utterance']\n",
    "            utterance=utterance.replace(\"   \",\" \")#In SNIPS i noticed a problem in the dataset, some sentences have two or morewhitespaces\n",
    "            utterance=utterance.replace(\"  \",\" \")#between words (example \"play a tune or two from kansas city  missouri\"  between city and missouri there are two whitespace)\n",
    "                                                #since bert tokenizer wrongly recognize ' ' as a token, I remove the extra whitespace while loading\n",
    "            self.utterances.append(utterance)\n",
    "            \n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        utterance = str(self.utterances[item])\n",
    "        lang=self.lang\n",
    "        intent = self.intent_ids[item]\n",
    "        slots = self.slot_ids[item]\n",
    "        slots_len = self.max_len,\n",
    "        sent_len = len(slots)\n",
    "        for i in range(sent_len,self.max_len):\n",
    "            slots.append(PAD_TOKEN)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          utterance,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'utterance': utterance,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),#'targets': torch.tensor(target, dtype=torch.long)\n",
    "          'intent': intent,\n",
    "          'slots_len': slots_len,\n",
    "          'slots': torch.tensor(slots, dtype=torch.long)  \n",
    "        }\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60d32e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_corpus = ATIS_tmp_train_raw + ATIS_test_raw \n",
    "ATIS_slots = set(sum([line['slots'].split() for line in ATIS_corpus],[]))\n",
    "ATIS_intents = set([line['intent'] for line in ATIS_corpus])\n",
    "\n",
    "ATIS_lang = BertLang(ATIS_intents, ATIS_slots)\n",
    "\n",
    "ATIS_train_dataset = BERTDataset(ATIS_train_raw, ATIS_lang, tokenizer, max_length)\n",
    "ATIS_dev_dataset = BERTDataset(ATIS_dev_raw, ATIS_lang, tokenizer, max_length)\n",
    "ATIS_test_dataset = BERTDataset(ATIS_test_raw, ATIS_lang, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8dff67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPS_corpus = SNIPS_tmp_train_raw + SNIPS_test_raw \n",
    "SNIPS_slots = set(sum([line['slots'].split() for line in SNIPS_corpus],[]))\n",
    "SNIPS_intents = set([line['intent'] for line in SNIPS_corpus])\n",
    "\n",
    "SNIPS_lang = BertLang(SNIPS_intents, SNIPS_slots)\n",
    "\n",
    "SNIPS_train_dataset = BERTDataset(SNIPS_train_raw, SNIPS_lang, tokenizer, max_length)\n",
    "SNIPS_dev_dataset = BERTDataset(SNIPS_dev_raw, SNIPS_lang, tokenizer, max_length)\n",
    "SNIPS_test_dataset = BERTDataset(SNIPS_test_raw, SNIPS_lang, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19649a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ATIS_train_dataset.__getitem__(456))\n",
    "\n",
    "#input id traduce la frase con il vocab di bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0b04368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATIS\n",
      "Train:  4384\n",
      "Dev:  600\n",
      "Test:  896\n",
      "=====================================================================================\n",
      "SNIPS\n",
      "Train:  11648\n",
      "Dev:  1440\n",
      "Test:  704\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader(dataset,batch_size):\n",
    "    return DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "ATIS_train_dataloader = create_data_loader(ATIS_train_dataset, train_batch_size)\n",
    "ATIS_dev_dataloader = create_data_loader(ATIS_dev_dataset, dev_batch_size)\n",
    "ATIS_test_dataloader = create_data_loader(ATIS_test_dataset, test_batch_size)\n",
    "\n",
    "SNIPS_train_dataloader = create_data_loader(SNIPS_train_dataset, train_batch_size)\n",
    "SNIPS_dev_dataloader = create_data_loader(SNIPS_dev_dataset, dev_batch_size)\n",
    "SNIPS_test_dataloader = create_data_loader(SNIPS_test_dataset, test_batch_size)\n",
    "\n",
    "#print(len(ATIS_train_dataloader),train_batch_size)\n",
    "print(\"ATIS\")\n",
    "print(\"Train: \",len(ATIS_train_dataloader)*train_batch_size)\n",
    "print(\"Dev: \",len(ATIS_dev_dataloader)*dev_batch_size)\n",
    "print(\"Test: \",len(ATIS_test_dataloader)*test_batch_size)\n",
    "print(\"=\"*85)\n",
    "print(\"SNIPS\")\n",
    "print(\"Train: \",len(SNIPS_train_dataloader)*train_batch_size)\n",
    "print(\"Dev: \",len(SNIPS_dev_dataloader)*dev_batch_size)\n",
    "print(\"Test: \",len(SNIPS_test_dataloader)*test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d016d",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fccc8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertJoint(nn.Module):\n",
    "    def __init__(self,hid_size, out_slot, out_int):\n",
    "        super(BertJoint,self).__init__()\n",
    "        self.bert_encoder= BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        \n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        #self.dropout=nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs= self.bert_encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        #returns sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1] \n",
    "\n",
    "        intents = self.intent_out(pooled_output)\n",
    "        slots = self.slot_out(sequence_output)\n",
    "        \n",
    "        #Slot size: batch size, seq len, classes\n",
    "        slots = slots.permute(0,2,1) #Used for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        \n",
    "        return slots, intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd8fc4",
   "metadata": {},
   "source": [
    "### Training Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9f169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "ATIS_criterion_intents = nn.CrossEntropyLoss()\n",
    "\n",
    "SNIPS_criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "SNIPS_criterion_intents = nn.CrossEntropyLoss()\n",
    "\n",
    "ATIS_out_slot = len(ATIS_lang.slot2id)\n",
    "ATIS_out_int = len(ATIS_lang.intent2id)\n",
    "ATIS_vocab_len = len(ATIS_lang.word2id)\n",
    "\n",
    "SNIPS_out_slot = len(SNIPS_lang.slot2id)\n",
    "SNIPS_out_int = len(SNIPS_lang.intent2id)\n",
    "SNIPS_vocab_len = len(SNIPS_lang.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "506255c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertJoint(\n",
       "  (bert_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (slot_out): Linear(in_features=768, out_features=130, bias=True)\n",
       "  (intent_out): Linear(in_features=768, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ATIS\n",
    "ATIS_model=BertJoint(hidden_size,ATIS_out_slot,ATIS_out_int)\n",
    "ATIS_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73e30ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertJoint(\n",
       "  (bert_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (slot_out): Linear(in_features=768, out_features=73, bias=True)\n",
       "  (intent_out): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SNIPS\n",
    "SNIPS_model=BertJoint(hidden_size,SNIPS_out_slot,SNIPS_out_int)\n",
    "SNIPS_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb254483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS\n",
    "ATIS_optimizer= torch.optim.AdamW(params=ATIS_model.parameters(),lr=learning_rate)\n",
    "ATIS_total_steps=len(ATIS_train_dataloader)*epochs\n",
    "ATIS_scheduler=transformers.get_linear_schedule_with_warmup(ATIS_optimizer,num_warmup_steps=0,num_training_steps=ATIS_total_steps)\n",
    "\n",
    "#SNIPS\n",
    "SNIPS_optimizer= torch.optim.AdamW(params=SNIPS_model.parameters(),lr=learning_rate)\n",
    "SNIPS_total_steps=len(SNIPS_train_dataloader)*epochs\n",
    "SNIPS_scheduler=transformers.get_linear_schedule_with_warmup(SNIPS_optimizer,num_warmup_steps=0,num_training_steps=SNIPS_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f69469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,intent_criterion,slots_criterion,optimizer,device,scheduler):\n",
    "    \n",
    "    model=model.train()\n",
    "    \n",
    "    loss_array = []\n",
    "    \n",
    "    for d in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_id=d['input_ids'].to(device)\n",
    "        attention_mask=d['attention_mask'].to(device)\n",
    "        intent_gt=d['intent'].to(device)\n",
    "        slot_gt=d['slots'].to(device)\n",
    "        \n",
    "        slot_pred,intent_pred=model(input_ids=input_id, attention_mask=attention_mask)\n",
    "        \n",
    "        loss_intent=intent_criterion(intent_pred,intent_gt)\n",
    "        \n",
    "        loss_slot=slots_criterion(slot_pred,slot_gt)\n",
    "        \n",
    "        #loss=loss_intent+loss_slot\n",
    "        \n",
    "        alpha=random.uniform(0, 1)\n",
    "        beta=1-alpha\n",
    "        \n",
    "        loss=max(alpha,beta) * loss_slot + min(alpha,beta) * loss_intent\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        optimizer.step() # Update the weights\n",
    "        scheduler.step()\n",
    "        \n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e5ce6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert(model, data_loader, intent_criterion,slots_criterion, lang,device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    y_slots = []\n",
    "    hyp_slots = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in data_loader:\n",
    "            input_ids = sample[\"input_ids\"].to(device)\n",
    "            attention_mask = sample[\"attention_mask\"].to(device)\n",
    "            ref_slots = sample[\"slots\"].to(device)\n",
    "            ref_intent = sample[\"intent\"].to(device)\n",
    "\n",
    "            slots, intents = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            \n",
    "            loss_intent = intent_criterion(intents, ref_intent)\n",
    "            loss_slot = slots_criterion(slots, ref_slots)\n",
    "            loss = loss_intent + loss_slot\n",
    "            \n",
    "            loss_array.append(loss.item())\n",
    "            \n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intent'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1) #Returns the indices of the maximum value of all elements in the input tensor.\n",
    "            ref_slots=ref_slots.tolist()\n",
    "            slots_len=len(ref_slots)\n",
    "            \n",
    "            \n",
    "            for id_seq, seq in enumerate(output_slots):#id_seq è il sample, seq è l'indice dello slot(id dello slot)\n",
    "                length = sample['slots_len'][0].data[0].item()\n",
    "                utterance = sample['utterance'][id_seq].split(\" \")\n",
    "                \n",
    "                gt_ids = sample['slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids]\n",
    "                \n",
    "                #Compute\n",
    "                tmp_ref=[]\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_ref.append((utterance[id_el], gt_slots[id_el]))\n",
    "                y_slots.append(tmp_ref)\n",
    "                \n",
    "                \n",
    "                slot_ids=seq.tolist()\n",
    "                \n",
    "                tmp_seq = []\n",
    "                for id_el in range(0,length):\n",
    "                    if(id_el<len(utterance)):\n",
    "                        tmp_seq.append((utterance[id_el], lang.id2slot[slot_ids[id_el]]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(y_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d14351",
   "metadata": {},
   "source": [
    "### Save and load model methods\n",
    "Since Bert training is more heavy on the hardware (15~20 minutes for 10 epochs on my setup for one dataset) I save trained models locally and load them for evaluation and error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3235e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_MODELS=\"bert_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "268c8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bert(model,atis):\n",
    "    #model --> model to save\n",
    "    #atis=true --> model is trained on atis dataset\n",
    "    #atis=false --> model is trained on snips dataset\n",
    "    if(atis):\n",
    "        model_name=\"ATIS\"\n",
    "    else:\n",
    "        model_name=\"SNIPS\"\n",
    "    path=os.path.join(DIR_MODELS,model_name)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(\"Model saved in\",path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7d5070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert(atis):\n",
    "    #atis=true --> load atis model\n",
    "    #atis=false --> load snips model\n",
    "    hid_size=hidden_size\n",
    "    if(atis):\n",
    "        model_name=\"atis_bert.pt\"\n",
    "        out_slot=ATIS_out_slot\n",
    "        out_int=ATIS_out_int\n",
    "    else:\n",
    "        model_name=\"snips_bert.pt\"\n",
    "        out_slot=SNIPS_out_slot\n",
    "        out_int=SNIPS_out_int\n",
    "    path=os.path.join(DIR_MODELS,model_name)\n",
    "    print(\"Loading model from\",path)\n",
    "    \n",
    "    model=BertJoint(hidden_size,out_slot,out_int).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad6713",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16870dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 10%|███████▉                                                                       | 10.0/100 [01:03<09:35,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss 0.718723860344965\n",
      "Dev Slot F1:  0.8583647798742139\n",
      "Dev Intent Accuracy: 0.9547738693467337\n",
      "Test Slot F1:  0.8159660897209465\n",
      "Test Intent Accuracy: 0.8779395296752519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▊                                                               | 20.0/100 [02:07<08:31,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss 0.21017269927491672\n",
      "Dev Slot F1:  0.93158953722334\n",
      "Dev Intent Accuracy: 0.966499162479062\n",
      "Test Slot F1:  0.8848996832101373\n",
      "Test Intent Accuracy: 0.8868980963045913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████▋                                                       | 30.0/100 [03:11<07:25,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train loss 0.11174532259223018\n",
      "Dev Slot F1:  0.9483967935871744\n",
      "Dev Intent Accuracy: 0.9782244556113903\n",
      "Test Slot F1:  0.8985405310356954\n",
      "Test Intent Accuracy: 0.9428891377379619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████▌                                               | 40.0/100 [04:14<06:20,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train loss 0.06966114840062376\n",
      "Dev Slot F1:  0.9514514514514515\n",
      "Dev Intent Accuracy: 0.9899497487437185\n",
      "Test Slot F1:  0.9096966508854988\n",
      "Test Intent Accuracy: 0.973124300111982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████▌                                       | 50.0/100 [05:17<05:17,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train loss 0.044197611806465545\n",
      "Dev Slot F1:  0.9574787393696848\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.9170320996316437\n",
      "Test Intent Accuracy: 0.975363941769317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████▍                               | 60.0/100 [06:20<04:13,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train loss 0.03207153134918126\n",
      "Dev Slot F1:  0.9676286072772897\n",
      "Dev Intent Accuracy: 0.9882747068676717\n",
      "Test Slot F1:  0.9248412138320397\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████▎                       | 70.0/100 [07:26<03:12,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train loss 0.022345726652730283\n",
      "Dev Slot F1:  0.9666248431618569\n",
      "Dev Intent Accuracy: 0.9865996649916248\n",
      "Test Slot F1:  0.9231311706629056\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████▏               | 80.0/100 [08:30<02:08,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train loss 0.017234322632387635\n",
      "Dev Slot F1:  0.9669172932330826\n",
      "Dev Intent Accuracy: 0.9882747068676717\n",
      "Test Slot F1:  0.9249735822472702\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████        | 90.0/100 [09:33<01:03,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train loss 0.01312708469272598\n",
      "Dev Slot F1:  0.9684052156469408\n",
      "Dev Intent Accuracy: 0.9882747068676717\n",
      "Test Slot F1:  0.9246213455442057\n",
      "Test Intent Accuracy: 0.9764837625979843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100.0/100 [10:37<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train loss 0.009858585001023854\n",
      "Dev Slot F1:  0.9691497366440932\n",
      "Dev Intent Accuracy: 0.9882747068676717\n",
      "Test Slot F1:  0.9258149779735684\n",
      "Test Intent Accuracy: 0.975363941769317\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in bert_models\\atis_bert.pt\n"
     ]
    }
   ],
   "source": [
    "#ATIS\n",
    "warnings.filterwarnings('ignore')#Used to ignore a warning from hugging face tokenizer that would get repeated at every epoch, gets displayed only firstime\n",
    "\n",
    "update_tick=100/epochs\n",
    "with tqdm(total=100) as pbar:#updating tqdm manually for clearer visualization\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_epoch(\n",
    "        ATIS_model,\n",
    "        ATIS_train_dataloader,\n",
    "        ATIS_criterion_intents,\n",
    "        ATIS_criterion_slots,\n",
    "        ATIS_optimizer,\n",
    "        device,\n",
    "        ATIS_scheduler)\n",
    "\n",
    "        results_dev, intent_res, loss_dev  = eval_bert(\n",
    "        ATIS_model,\n",
    "        ATIS_dev_dataloader,\n",
    "        ATIS_criterion_intents,\n",
    "        ATIS_criterion_slots,\n",
    "        ATIS_lang,\n",
    "        device)\n",
    "        \n",
    "        test_results_dev, test_intent_res, test_loss_dev  = eval_bert(\n",
    "        ATIS_model,\n",
    "        ATIS_test_dataloader,\n",
    "        ATIS_criterion_intents,\n",
    "        ATIS_criterion_slots,\n",
    "        ATIS_lang,\n",
    "        device)\n",
    "\n",
    "        print(\"Epoch:\",epoch)\n",
    "        print(f'Train loss {mean(train_loss)}')\n",
    "        print('Dev Slot F1: ', results_dev['total']['f'])\n",
    "        print('Dev Intent Accuracy:', intent_res['accuracy'])\n",
    "        \n",
    "        print('Test Slot F1: ', test_results_dev['total']['f'])\n",
    "        print('Test Intent Accuracy:', test_intent_res['accuracy'])\n",
    "\n",
    "        pbar.update(update_tick)#updating tqdm manually for clearer visualization\n",
    "\n",
    "print(\"Saving model...\")\n",
    "save_model(ATIS_model,atis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea22195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19910df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 10.0/100 [02:38<23:49, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss 0.5741822937667206\n",
      "Slot F1:  0.8309517390132111\n",
      "Intent Accuracy: 0.9826388888888888\n",
      "Test Slot F1:  0.8028993587956511\n",
      "Test Intent Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▊                                                               | 20.0/100 [05:22<21:32, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train loss 0.155387786009789\n",
      "Slot F1:  0.8762720942688806\n",
      "Intent Accuracy: 0.9826388888888888\n",
      "Test Slot F1:  0.8514742353265362\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████▋                                                       | 30.0/100 [08:04<18:53, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train loss 0.09259291354612867\n",
      "Slot F1:  0.9124214467174756\n",
      "Intent Accuracy: 0.9888888888888889\n",
      "Test Slot F1:  0.8845941468801768\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████▌                                               | 40.0/100 [10:43<16:04, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train loss 0.06059806827765347\n",
      "Slot F1:  0.9114534494306765\n",
      "Intent Accuracy: 0.9881944444444445\n",
      "Test Slot F1:  0.8981737686773659\n",
      "Test Intent Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████▌                                       | 50.0/100 [13:23<13:22, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train loss 0.03878593348765587\n",
      "Slot F1:  0.9255091103965701\n",
      "Intent Accuracy: 0.9902777777777778\n",
      "Test Slot F1:  0.9083863825076114\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████▍                               | 60.0/100 [16:06<10:45, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train loss 0.027303120336360374\n",
      "Slot F1:  0.9224993295789757\n",
      "Intent Accuracy: 0.9888888888888889\n",
      "Test Slot F1:  0.9155086158977209\n",
      "Test Intent Accuracy: 0.9814285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████████████████████████████▎                       | 70.0/100 [18:51<08:07, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train loss 0.016959556388098793\n",
      "Slot F1:  0.9310576535411907\n",
      "Intent Accuracy: 0.9909722222222223\n",
      "Test Slot F1:  0.9162506932889629\n",
      "Test Intent Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████▏               | 80.0/100 [21:33<05:24, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train loss 0.010071802263085444\n",
      "Slot F1:  0.9364224137931035\n",
      "Intent Accuracy: 0.9895833333333334\n",
      "Test Slot F1:  0.9252649191299498\n",
      "Test Intent Accuracy: 0.9842857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████        | 90.0/100 [24:12<02:41, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train loss 0.006681728353234556\n",
      "Slot F1:  0.9341751746372918\n",
      "Intent Accuracy: 0.9895833333333334\n",
      "Test Slot F1:  0.920935412026726\n",
      "Test Intent Accuracy: 0.9828571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100.0/100 [26:51<00:00, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train loss 0.004288000444720747\n",
      "Slot F1:  0.9354795439302481\n",
      "Intent Accuracy: 0.9909722222222223\n",
      "Test Slot F1:  0.9254727474972192\n",
      "Test Intent Accuracy: 0.9814285714285714\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in bert_models\\snips_bert.pt\n"
     ]
    }
   ],
   "source": [
    "#SNIPS\n",
    "warnings.filterwarnings('ignore')#Used to ignore a warning from hugging face tokenizer that would get repeated at every epoch, gets displayed only firstime\n",
    "\n",
    "update_tick=100/epochs\n",
    "with tqdm(total=100) as pbar:#updating tqdm manually for clearer visualization\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_epoch(\n",
    "        SNIPS_model,\n",
    "        SNIPS_train_dataloader,\n",
    "        SNIPS_criterion_intents,\n",
    "        SNIPS_criterion_slots,\n",
    "        SNIPS_optimizer,\n",
    "        device,\n",
    "        SNIPS_scheduler)\n",
    "\n",
    "\n",
    "        results_dev, intent_res, loss_dev  = eval_bert(\n",
    "        SNIPS_model,\n",
    "        SNIPS_dev_dataloader,\n",
    "        SNIPS_criterion_intents,\n",
    "        SNIPS_criterion_slots,\n",
    "        SNIPS_lang,\n",
    "        device)\n",
    "\n",
    "        test_results_dev, test_intent_res, test_loss_dev  = eval_bert(\n",
    "            SNIPS_model,\n",
    "            SNIPS_test_dataloader,\n",
    "            SNIPS_criterion_intents,\n",
    "            SNIPS_criterion_slots,\n",
    "            SNIPS_lang,\n",
    "            device)\n",
    "    \n",
    "    \n",
    "        print(\"Epoch:\",epoch)\n",
    "        print(f'Train loss {mean(train_loss)}')\n",
    "        print('Slot F1: ',results_dev['total']['f'])\n",
    "        print('Intent Accuracy:', intent_res['accuracy'])\n",
    "\n",
    "        print('Test Slot F1: ', test_results_dev['total']['f'])\n",
    "        print('Test Intent Accuracy:', test_intent_res['accuracy'])\n",
    "\n",
    "        pbar.update(update_tick)#updating tqdm manually for clearer visualization\n",
    "    \n",
    "print(\"Saving model...\")\n",
    "save_model(SNIPS_model,atis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ca8e3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd860",
   "metadata": {},
   "source": [
    "Il load da checkpoint non funziona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81933cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved bert models\n",
    "\n",
    "ATIS_bert=load_model(atis=True)\n",
    "SNIPS_bert=load_model(atis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce968b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATIS_results_test, ATIS_intent_test, _ = eval_bert(ATIS_bert, ATIS_test_dataloader, ATIS_criterion_intents, \n",
    "                                                   ATIS_criterion_slots, ATIS_lang,device)\n",
    "print(\"ATIS\")\n",
    "print('Slot F1: ',ATIS_results_test['total']['f'])\n",
    "print('Intent Accuracy:', ATIS_intent_test['accuracy'])\n",
    "\n",
    "\n",
    "SNIPS_results_test, SNIPS_intent_test, _ = eval_bert(SNIPS_bert, SNIPS_test_dataloader, SNIPS_criterion_intents, \n",
    "                                                   SNIPS_criterion_slots, SNIPS_lang,device)\n",
    "\n",
    "print(\"SNIPS\")\n",
    "print('Slot F1: ',SNIPS_results_test['total']['f'])\n",
    "print('Intent Accuracy:', SNIPS_intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b76b9",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATIS\n",
    "intent_hyp,intent_gt,slot_hyp,slot_gt=predict(ATIS_test_loader, ATIS_criterion_slots,\n",
    "                                              ATIS_criterion_intents, ATIS_bigru, ATIS_lang)\n",
    "#Slot is array of array, intent is array of strings\n",
    "\n",
    "#Get an array of (predictions,ground truth) based on utterance length\n",
    "intent_acc_lenght(intent_hyp,intent_gt, ATIS=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
